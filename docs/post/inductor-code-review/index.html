<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Inductor code review | ch_huang</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="compile_fx dynamo 注册 inductor 主入口是 compile_fx 函数，在dynamo中 @register_backend def inductor(*args, **kwargs): # do import here to avoid loading inductor into memory when it is not used from torch._inductor.compile_fx import compile_fx return compile_fx(*args, **kwargs) compile_fx 核心逻辑 函数声明 def compile_fx( model_: torch.fx.GraphModule, example_inputs_: List[torch.Tensor], inner_compile: Callable[..., Any] = compile_fx_inner, config_patches: Optional[Dict[str, Any]]">
<meta name="generator" content="Hugo 0.68.3" />


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">



<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />




  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>





  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">←</span>Home</a>
	
	<a href="/posts">Archive</a>
	<a href="/tags">Tags</a>
	<a href="/about">About</a>

	

	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Inductor code review</h1>

    <div class="tip">
        <time datetime="2023-12-19 00:00:00 &#43;0000 UTC">Dec 19, 2023</time>
        
        <span class="split">
          ·
        </span>
        <span>
          20 minute read
        </span>
    </div>

    
    
        
  
    <aside class="toc">
      <details>
          <summary>Table of Contents
          </summary>
          <div>
              <nav id="TableOfContents">
  <ul>
    <li><a href="#dynamo-注册-inductor">dynamo 注册 inductor</a></li>
    <li><a href="#compile_fx-核心逻辑">compile_fx 核心逻辑</a>
      <ul>
        <li><a href="#函数声明">函数声明</a></li>
        <li><a href="#dynamo之外的情形的处理">dynamo之外的情形的处理</a></li>
        <li><a href="#推理训练通用的-forward-graph-的compiler">推理/训练通用的 forward graph 的compiler</a></li>
        <li><a href="#定义-fw_compiler-inference_compiler-bw_compiler">定义 fw_compiler, inference_compiler, bw_compiler</a></li>
        <li><a href="#定义切分aot-autograd生成的joint-graph的函数">定义切分aot-autograd生成的joint-graph的函数</a></li>
        <li><a href="#调用aot_autograd进行计算图编译">调用aot_autograd进行计算图编译</a></li>
      </ul>
    </li>
    <li><a href="#compile_fx_inner">compile_fx_inner</a>
      <ul>
        <li><a href="#函数声明和一些非核心的处理逻辑">函数声明和一些非核心的处理逻辑</a></li>
        <li><a href="#调用fx_codegen_and_compile进行计算图编译">调用fx_codegen_and_compile进行计算图编译</a></li>
        <li><a href="#fx_codegen_and_compile">fx_codegen_and_compile</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#interpreter">Interpreter</a>
      <ul>
        <li><a href="#声明和注释">声明和注释</a></li>
        <li><a href="#run核心逻辑">run核心逻辑</a></li>
        <li><a href="#run_node">run_node</a></li>
        <li><a href="#call_function">call_function</a></li>
      </ul>
    </li>
    <li><a href="#graphlowering-1">GraphLowering</a>
      <ul>
        <li><a href="#run_node核心逻辑">run_node核心逻辑</a></li>
        <li><a href="#call_function-1">call_function</a></li>
        <li><a href="#compile_to_fn">compile_to_fn</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#__init__"><strong>init</strong></a></li>
    <li><a href="#fuse_nodes">fuse_nodes</a></li>
    <li><a href="#codegen">codegen</a></li>
    <li><a href="#tritonscheduling">TritonScheduling</a></li>
    <li><a href="#codegen_template">codegen_template</a></li>
    <li><a href="#codegen_extern_call">codegen_extern_call</a></li>
    <li><a href="#codegen_foreach">codegen_foreach</a></li>
    <li><a href="#codegen_nodes">codegen_nodes</a></li>
    <li><a href="#node">Node</a>
      <ul>
        <li><a href="#baseschedulernode">BaseSchedulerNode</a></li>
        <li><a href="#externkernelschedulernode">ExternKernelSchedulerNode</a></li>
        <li><a href="#schedulernode">SchedulerNode</a></li>
        <li><a href="#nopkernelschedulernode">NopKernelSchedulerNode</a></li>
        <li><a href="#fusedschedulernode">FusedSchedulerNode</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#lowerings">lowerings</a>
      <ul>
        <li><a href="#register_lowering">register_lowering</a></li>
        <li><a href="#make_pointwise">make_pointwise</a></li>
        <li><a href="#make_reduction">make_reduction</a></li>
        <li><a href="#xxxview">xxxView</a></li>
      </ul>
    </li>
    <li><a href="#realize">realize</a></li>
  </ul>

  <ul>
    <li><a href="#inductor-ir的设计">Inductor IR的设计</a></li>
    <li><a href="#realize-1">realize</a>
      <ul>
        <li><a href="#reduce-op-会进行realize">reduce op 会进行realize</a></li>
        <li><a href="#graphlowering-2">GraphLowering</a></li>
        <li><a href="#op-lowering">op lowering</a></li>
        <li><a href="#storageboxrealize">StorageBox.realize</a></li>
      </ul>
    </li>
    <li><a href="#loopbody">LoopBody</a></li>
  </ul>

  <ul>
    <li><a href="#core_aten_decompositions">core_aten_decompositions</a></li>
    <li><a href="#get_decompositions">get_decompositions</a></li>
  </ul>
</nav>
          </div>
      </details>
    </aside>
  


    


    <div class="content">
      <p><p class="markdown-image">
  <img src="/images/inductor.png" alt="inductor"  title="TorchInductor" />
</p></p>
<h1 id="compile_fx">compile_fx</h1>
<ul>
<li>
<h2 id="dynamo-注册-inductor">dynamo 注册 inductor</h2>
</li>
</ul>
<p>主入口是 compile_fx 函数，在dynamo中</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f">@register_backend</span>
<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">inductor</span>(<span style="color:#666">*</span>args, <span style="color:#666">**</span>kwargs):
    <span style="color:#080;font-style:italic"># do import here to avoid loading inductor into memory when it is not used</span>
    <span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">torch._inductor.compile_fx</span> <span style="color:#a2f;font-weight:bold">import</span> compile_fx

    <span style="color:#a2f;font-weight:bold">return</span> compile_fx(<span style="color:#666">*</span>args, <span style="color:#666">**</span>kwargs)
</code></pre></div><ul>
<li>
<h2 id="compile_fx-核心逻辑">compile_fx 核心逻辑</h2>
</li>
<li>
<h3 id="函数声明">函数声明</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">compile_fx</span>(
    model_: torch<span style="color:#666">.</span>fx<span style="color:#666">.</span>GraphModule,
    example_inputs_: List[torch<span style="color:#666">.</span>Tensor],
    inner_compile: Callable[<span style="color:#666">...</span>, Any] <span style="color:#666">=</span> compile_fx_inner,
    config_patches: Optional[Dict[<span style="color:#a2f">str</span>, Any]] <span style="color:#666">=</span> None,
    decompositions: Optional[Dict[OpOverload, Callable[<span style="color:#666">...</span>, Any]]] <span style="color:#666">=</span> None,
):
    <span style="color:#b44">&#34;&#34;&#34;Main entrypoint to a compile given FX graph&#34;&#34;&#34;</span>
</code></pre></div><ul>
<li>
<h3 id="dynamo之外的情形的处理">dynamo之外的情形的处理</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#080;font-style:italic"># NOTE. 根据传入的patches 去wrap inner_compile.</span>
    <span style="color:#a2f;font-weight:bold">if</span> config_patches:
        <span style="color:#a2f;font-weight:bold">with</span> config<span style="color:#666">.</span>patch(config_patches):  <span style="color:#080;font-style:italic"># type: ignore[attr-defined]</span>
            <span style="color:#a2f;font-weight:bold">return</span> compile_fx(
                model_,
                example_inputs_,
                <span style="color:#080;font-style:italic"># need extra layer of patching as backwards is compiled out of scope</span>
                inner_compile<span style="color:#666">=</span>config<span style="color:#666">.</span>patch(config_patches)(inner_compile),  <span style="color:#080;font-style:italic"># type: ignore[attr-defined]</span>
                decompositions<span style="color:#666">=</span>decompositions,
            )

    <span style="color:#080;font-style:italic"># NOTE. 生成 dynamic library，仅对cuda生效。`inner_compile_with_cpp_wrapper` 对 non-cuda 后端的branch，直接调用 inner_compile 完成编译；对 cuda 后端则先调用 inner_compile 编译并实际执行，然后将 cpp_wrapper 改成 True， 去生成 cpp wrapper code 以及生成动态库。</span>
    <span style="color:#a2f;font-weight:bold">if</span> config<span style="color:#666">.</span>cpp_wrapper:
        <span style="color:#a2f;font-weight:bold">with</span> config<span style="color:#666">.</span>patch(  <span style="color:#080;font-style:italic"># type: ignore[attr-defined]</span>
            {
                <span style="color:#b44">&#34;cpp_wrapper&#34;</span>: False,
                <span style="color:#b44">&#34;triton.autotune_cublasLt&#34;</span>: False,
                <span style="color:#b44">&#34;triton.cudagraphs&#34;</span>: False,
                <span style="color:#080;font-style:italic"># CudaWrapperCodeGen relies on kernel name to find the autotuned cubin file</span>
                <span style="color:#b44">&#34;triton.unique_kernel_names&#34;</span>: True,
            }
        ), V<span style="color:#666">.</span>set_real_inputs(
            example_inputs_
        ):  <span style="color:#080;font-style:italic"># type: ignore[call-arg]</span>
            <span style="color:#a2f;font-weight:bold">return</span> compile_fx(
                model_,
                example_inputs_,
                inner_compile<span style="color:#666">=</span>inner_compile_with_cpp_wrapper(inner_compile),
                decompositions<span style="color:#666">=</span>decompositions,
            )

    recursive_compile_fx <span style="color:#666">=</span> functools<span style="color:#666">.</span>partial(
        compile_fx,
        inner_compile<span style="color:#666">=</span>inner_compile,
        decompositions<span style="color:#666">=</span>decompositions,
    )

    <span style="color:#080;font-style:italic"># NOTE. 如果 gm 的返回值不是 tuple，则修改 gm 将其返回值改为tuple类型，然后返回 recursive_compile_fx 的编译结果。这是用于 non-dynamo 生成的计算图。 </span>
    <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f;font-weight:bold">not</span> graph_returns_tuple(model_):
        <span style="color:#a2f;font-weight:bold">return</span> make_graph_return_tuple(
            model_,
            example_inputs_,
            recursive_compile_fx,
        )

    <span style="color:#080;font-style:italic"># NOTE. 处理 dynamo.export 生成的fx graph 中嵌入的 pytrees，调用 codegen.process_inputs 使得 inductor可以正常编译这个 graph.</span>
    <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(model_, torch<span style="color:#666">.</span>fx<span style="color:#666">.</span>GraphModule):
        <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(model_<span style="color:#666">.</span>graph<span style="color:#666">.</span>_codegen, _PyTreeCodeGen):
            <span style="color:#080;font-style:italic"># this graph is the result of dynamo.export()</span>
            <span style="color:#a2f;font-weight:bold">return</span> handle_dynamo_export_graph(
                model_,
                example_inputs_,
                recursive_compile_fx,
            )

        <span style="color:#080;font-style:italic"># Since handle_dynamo_export_graph will trigger compile_fx again,</span>
        <span style="color:#080;font-style:italic"># Move these passes after handle_dynamo_export_graph to avoid repeated calls.</span>
        model_ <span style="color:#666">=</span> pre_grad_passes(model_, example_inputs_)

    <span style="color:#080;font-style:italic"># NOTE. 处理 non-dynamo 生成的 graph，inputs 没有被flat.</span>
    <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">any</span>(<span style="color:#a2f">isinstance</span>(x, (<span style="color:#a2f">list</span>, <span style="color:#a2f">tuple</span>, <span style="color:#a2f">dict</span>)) <span style="color:#a2f;font-weight:bold">for</span> x <span style="color:#a2f;font-weight:bold">in</span> example_inputs_):
        <span style="color:#a2f;font-weight:bold">return</span> flatten_graph_inputs(
            model_,
            example_inputs_,
            recursive_compile_fx,
        )
</code></pre></div><ul>
<li>
<h3 id="推理训练通用的-forward-graph-的compiler">推理/训练通用的 forward graph 的compiler</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#080;font-style:italic"># NOTE. 区别处理推理场景与训练场景的逻辑.</span>
    <span style="color:#a2f">@dynamo_utils.dynamo_timed</span>
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">fw_compiler_base</span>(
        model: torch<span style="color:#666">.</span>fx<span style="color:#666">.</span>GraphModule,
        example_inputs: List[torch<span style="color:#666">.</span>Tensor],
        is_inference: <span style="color:#a2f">bool</span>,
    ):
        <span style="color:#080;font-style:italic"># NOTE. 对推理场景做优化。尝试（1）常量折叠（2）替换随机函数（3）计算图拓扑序重排，graph重新生成，去除autograd生成的死节点。</span>
        <span style="color:#a2f;font-weight:bold">if</span> is_inference:
            <span style="color:#080;font-style:italic"># partition_fn won&#39;t be called</span>
            joint_graph_passes(model)

        num_rng_seed_offset_inputs <span style="color:#666">=</span> <span style="color:#666">2</span> <span style="color:#a2f;font-weight:bold">if</span> functorch_config<span style="color:#666">.</span>functionalize_rng_ops <span style="color:#a2f;font-weight:bold">else</span> <span style="color:#666">0</span>
        fixed <span style="color:#666">=</span> <span style="color:#a2f">len</span>(example_inputs) <span style="color:#666">-</span> num_example_inputs <span style="color:#666">-</span> num_rng_seed_offset_inputs
        user_visible_outputs <span style="color:#666">=</span> <span style="color:#a2f">set</span>()

        <span style="color:#080;font-style:italic"># NOTE. 是否保持outputs的stride，如果是单纯的推理场景，一般不太需要保持。</span>
        <span style="color:#a2f;font-weight:bold">if</span> config<span style="color:#666">.</span>keep_output_stride:
            <span style="color:#666">*</span>_, model_outputs_node <span style="color:#666">=</span> model<span style="color:#666">.</span>graph<span style="color:#666">.</span>nodes
            <span style="color:#a2f;font-weight:bold">assert</span> model_outputs_node<span style="color:#666">.</span>op <span style="color:#666">==</span> <span style="color:#b44">&#34;output&#34;</span>
            model_outputs, _ <span style="color:#666">=</span> pytree<span style="color:#666">.</span>tree_flatten(model_outputs_node<span style="color:#666">.</span>args)
            num_model_outputs <span style="color:#666">=</span> <span style="color:#a2f">len</span>(model_outputs)

            context <span style="color:#666">=</span> torch<span style="color:#666">.</span>_guards<span style="color:#666">.</span>TracingContext<span style="color:#666">.</span>get()
            <span style="color:#080;font-style:italic"># NOTE. joint-graph的outputs由原始计算图的outputs和保存给反向图的inputs组成，一般原始outputs都是在outputs的最前面，但是 aot-autograd 会把inplace updated tensors 放在最前面，这导致原始outputs的位置难以明确，所以需要用`original_output_start_index`去指明这个位置（根据mutated inputs数量即可获得）。</span>
            <span style="color:#a2f;font-weight:bold">if</span> context <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> None <span style="color:#a2f;font-weight:bold">and</span> context<span style="color:#666">.</span>fw_metadata:
                original_output_start_index <span style="color:#666">=</span> context<span style="color:#666">.</span>fw_metadata<span style="color:#666">.</span>num_mutated_inputs
            <span style="color:#a2f;font-weight:bold">else</span>:
                original_output_start_index <span style="color:#666">=</span> <span style="color:#666">0</span>

            <span style="color:#080;font-style:italic"># NOTE. 获取原始outputs的数量.</span>
            <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(model_, torch<span style="color:#666">.</span>fx<span style="color:#666">.</span>GraphModule):
                <span style="color:#666">*</span>_, orig_model_outputs_node <span style="color:#666">=</span> model_<span style="color:#666">.</span>graph<span style="color:#666">.</span>nodes
                <span style="color:#a2f;font-weight:bold">assert</span> orig_model_outputs_node<span style="color:#666">.</span>op <span style="color:#666">==</span> <span style="color:#b44">&#34;output&#34;</span>
                orig_model_outputs, _ <span style="color:#666">=</span> pytree<span style="color:#666">.</span>tree_flatten(
                    orig_model_outputs_node<span style="color:#666">.</span>args
                )
                num_orig_model_outputs <span style="color:#666">=</span> <span style="color:#a2f">len</span>(orig_model_outputs)
            <span style="color:#a2f;font-weight:bold">else</span>:
                num_orig_model_outputs <span style="color:#666">=</span> num_model_outputs

            <span style="color:#a2f;font-weight:bold">assert</span> num_orig_model_outputs <span style="color:#666">&lt;=</span> num_model_outputs

            <span style="color:#080;font-style:italic"># We makes the following assumption</span>
            <span style="color:#080;font-style:italic"># For inference</span>
            <span style="color:#080;font-style:italic">#   len(orig_model_outputs) == len(model_outputs)</span>
            <span style="color:#080;font-style:italic"># For training</span>
            <span style="color:#080;font-style:italic">#   len(orig_model_outputs) &lt;= len(model_outputs)</span>
            <span style="color:#080;font-style:italic"># During training, most of the time the model_outputs starts with</span>
            <span style="color:#080;font-style:italic"># orignal module&#39;s outputs followed by saved activations.</span>
            <span style="color:#080;font-style:italic"># But this can be not true if the model have inplace updated tensors.</span>
            <span style="color:#080;font-style:italic"># AOTAutograd will make those tensors being returned before the orignal</span>
            <span style="color:#080;font-style:italic"># module&#39;s output.</span>
            <span style="color:#080;font-style:italic"># To make things safe, we&#39;ll use original_output_start_index field</span>
            <span style="color:#080;font-style:italic"># set by AOTAutograd to decide where the original module outputs start.</span>

            <span style="color:#080;font-style:italic"># NOTE. 遍历原始的outputs，填入`user_visible_outputs`，`GraphLowering`会根据这个set去判断是否考虑 keep outputs stride.</span>
            user_visible_outputs <span style="color:#666">=</span> {
                n<span style="color:#666">.</span>name
                <span style="color:#a2f;font-weight:bold">for</span> n <span style="color:#a2f;font-weight:bold">in</span> model_outputs[
                    original_output_start_index : original_output_start_index
                    <span style="color:#666">+</span> num_orig_model_outputs
                ]
                <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(n, torch<span style="color:#666">.</span>fx<span style="color:#666">.</span>Node)
            }

        <span style="color:#a2f;font-weight:bold">return</span> inner_compile(
            model,
            example_inputs,
            num_fixed<span style="color:#666">=</span>fixed,
            cudagraphs<span style="color:#666">=</span>cudagraphs,
            graph_id<span style="color:#666">=</span>graph_id,
            is_inference<span style="color:#666">=</span>is_inference,
            boxed_forward_device_index<span style="color:#666">=</span>forward_device,
            user_visible_outputs<span style="color:#666">=</span>user_visible_outputs,
        )
</code></pre></div><ul>
<li>
<h3 id="定义-fw_compiler-inference_compiler-bw_compiler">定义 fw_compiler, inference_compiler, bw_compiler</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#080;font-style:italic"># NOTE. fw_compiler 是用于训练的。</span>
    fw_compiler <span style="color:#666">=</span> functools<span style="color:#666">.</span>partial(fw_compiler_base, is_inference<span style="color:#666">=</span>False)

    <span style="color:#080;font-style:italic"># NOTE. `inference_compiler` 用于推理.</span>
    <span style="color:#080;font-style:italic"># NOTE. 如果开启了freezing功能且当前上下文关闭了grad，则生成infer用的compiler，其会调用 freeze 做常量折叠优化，算子layout优化等. 否则直接偏特化 fw_compiler_base</span>
    <span style="color:#a2f;font-weight:bold">if</span> config<span style="color:#666">.</span>freezing <span style="color:#a2f;font-weight:bold">and</span> <span style="color:#a2f;font-weight:bold">not</span> torch<span style="color:#666">.</span>is_grad_enabled():
        inference_compiler <span style="color:#666">=</span> functools<span style="color:#666">.</span>partial(
            fw_compiler_freezing,
            dynamo_model<span style="color:#666">=</span>model_,
            num_example_inputs<span style="color:#666">=</span>num_example_inputs,
            inner_compile<span style="color:#666">=</span>inner_compile,
            cudagraphs<span style="color:#666">=</span>cudagraphs,
            graph_id<span style="color:#666">=</span>graph_id,
            forward_device<span style="color:#666">=</span>forward_device,
        )
    <span style="color:#a2f;font-weight:bold">else</span>:
        inference_compiler <span style="color:#666">=</span> functools<span style="color:#666">.</span>partial(fw_compiler_base, is_inference<span style="color:#666">=</span>True)
        
    <span style="color:#a2f">@dynamo_utils.dynamo_timed</span>
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">bw_compiler</span>(model: torch<span style="color:#666">.</span>fx<span style="color:#666">.</span>GraphModule, example_inputs: List[torch<span style="color:#666">.</span>Tensor]):
        fixed <span style="color:#666">=</span> count_tangents(model)
        <span style="color:#a2f;font-weight:bold">return</span> inner_compile(
            model,
            example_inputs,
            num_fixed<span style="color:#666">=</span>fixed,
            cudagraphs<span style="color:#666">=</span>cudagraphs,
            is_backward<span style="color:#666">=</span>True,
            graph_id<span style="color:#666">=</span>graph_id,
            boxed_forward_device_index<span style="color:#666">=</span>forward_device,
        )
</code></pre></div><ul>
<li>
<h3 id="定义切分aot-autograd生成的joint-graph的函数">定义切分aot-autograd生成的joint-graph的函数</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#080;font-style:italic"># NOTE. 该函数用于在训练中，对 aot-autograd 产生的 joint graph 进行切分，使用 min-cut-max-flow 算法对graph切分，用 recomputing 的代价减少内存开销（反向图的很多输入是正向图的中间结果，一般情况下需要对中间节点进行保存，以给反向图计算用，采用重计算的方式减少中间节点）。</span>
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">partition_fn</span>(graph, joint_inputs, <span style="color:#666">**</span>kwargs):
        joint_graph_passes(graph)
        <span style="color:#a2f;font-weight:bold">return</span> min_cut_rematerialization_partition(
            graph, joint_inputs, <span style="color:#666">**</span>kwargs, compiler<span style="color:#666">=</span><span style="color:#b44">&#34;inductor&#34;</span>
        )
</code></pre></div><ul>
<li>
<h3 id="调用aot_autograd进行计算图编译">调用aot_autograd进行计算图编译</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#080;font-style:italic"># NOTE. 编译阶段需要在 fake mode下执行，以正确的做符号执行。`detect_fake_mode`从inputs推导当前graph的fake mode，如果没有推出来那么构造允许实例inputs作为输入的fake mode。在fake mode下，torch在调度op实现时会调度到FakeTensorMode下的diapatch，从而进行符号执行。如果没有强制使用fake-mode，torch也会根据inputs/outputs类型去推导调度到哪个后端，简单的情形下也是可行的，但是无法正确处理没有inputs的op。</span>

    <span style="color:#080;font-style:italic"># TODO: can add logging before/after the call to create_aot_dispatcher_function</span>
    <span style="color:#080;font-style:italic"># in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func</span>
    <span style="color:#080;font-style:italic"># once torchdynamo is merged into pytorch</span>
    fake_mode <span style="color:#666">=</span> detect_fake_mode(example_inputs_) <span style="color:#a2f;font-weight:bold">or</span> torch<span style="color:#666">.</span>_subclasses<span style="color:#666">.</span>FakeTensorMode(
        allow_non_fake_inputs<span style="color:#666">=</span>True
    )
    tracing_context <span style="color:#666">=</span> (
        torch<span style="color:#666">.</span>_guards<span style="color:#666">.</span>TracingContext<span style="color:#666">.</span>get() <span style="color:#a2f;font-weight:bold">or</span> torch<span style="color:#666">.</span>_guards<span style="color:#666">.</span>TracingContext(fake_mode)
    )

    <span style="color:#a2f;font-weight:bold">with</span> V<span style="color:#666">.</span>set_fake_mode(fake_mode), torch<span style="color:#666">.</span>_guards<span style="color:#666">.</span>tracing(  <span style="color:#080;font-style:italic"># type: ignore[call-arg]</span>
        tracing_context
    ), compiled_autograd<span style="color:#666">.</span>disable():
        <span style="color:#080;font-style:italic"># NOTE. `aot_autograd`对graph进行反向图构建，调用`partition`对joint-graph进行切分，调用fw_compiler/bw_compiler 对切分出的fw/bw graph 进行编译，另外也会使用`inference_compiler`对推理场景的fw graph进行编译.</span>
        <span style="color:#a2f;font-weight:bold">return</span> aot_autograd(
            fw_compiler<span style="color:#666">=</span>fw_compiler,
            bw_compiler<span style="color:#666">=</span>bw_compiler,
            inference_compiler<span style="color:#666">=</span>inference_compiler,
            decompositions<span style="color:#666">=</span>decompositions,
            partition_fn<span style="color:#666">=</span>partition_fn,
            keep_inference_input_mutations<span style="color:#666">=</span>True,
        )(model_, example_inputs_)
</code></pre></div><ul>
<li>
<h2 id="compile_fx_inner">compile_fx_inner</h2>
</li>
</ul>
<p>inductor对fw/bw/infer graph 的编译都是在<code>compile_fx_inne</code>r这个函数实现的。</p>
<ul>
<li>
<h3 id="函数声明和一些非核心的处理逻辑">函数声明和一些非核心的处理逻辑</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f">@DebugContext.wrap</span>
<span style="color:#a2f">@torch.utils._python_dispatch._disable_current_modes</span>()
<span style="color:#a2f">@time_and_log</span>(attr<span style="color:#666">=</span><span style="color:#b44">&#34;compilation time (in seconds)&#34;</span>)
<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">compile_fx_inner</span>(
    gm: torch<span style="color:#666">.</span>fx<span style="color:#666">.</span>GraphModule,
    example_inputs: List[torch<span style="color:#666">.</span>Tensor],
    cudagraphs: Optional[BoxedBool] <span style="color:#666">=</span> None,
    num_fixed: <span style="color:#a2f">int</span> <span style="color:#666">=</span> <span style="color:#666">0</span>,
    is_backward: <span style="color:#a2f">bool</span> <span style="color:#666">=</span> False,
    graph_id: Optional[<span style="color:#a2f">int</span>] <span style="color:#666">=</span> None,
    cpp_wrapper: <span style="color:#a2f">bool</span> <span style="color:#666">=</span> False,
    aot_mode: <span style="color:#a2f">bool</span> <span style="color:#666">=</span> False,
    is_inference: <span style="color:#a2f">bool</span> <span style="color:#666">=</span> False,
    boxed_forward_device_index: Optional[BoxedDeviceIndex] <span style="color:#666">=</span> None,
    user_visible_outputs: FrozenSet[<span style="color:#a2f">str</span>] <span style="color:#666">=</span> <span style="color:#a2f">frozenset</span>(),
    layout_opt: Optional[<span style="color:#a2f">bool</span>] <span style="color:#666">=</span> None,
):
    <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">    Inductor API that compiles a single graph.
</span><span style="color:#b44">
</span><span style="color:#b44">    If you change the argument list for this funtion, make sure you
</span><span style="color:#b44">    also update the call to save_args_for_compile_fx_inner below accordingly.
</span><span style="color:#b44">    &#34;&#34;&#34;</span>
    <span style="color:#080;font-style:italic"># NOTE. 如果graph中call op的数量为0，则直接返回，不需要编译优化.</span>
    <span style="color:#a2f;font-weight:bold">if</span> dynamo_utils<span style="color:#666">.</span>count_calls(gm<span style="color:#666">.</span>graph) <span style="color:#666">==</span> <span style="color:#666">0</span>:
        <span style="color:#a2f;font-weight:bold">return</span> make_boxed_func(gm<span style="color:#666">.</span>forward)

    <span style="color:#080;font-style:italic"># NOTE. `save_args_for_compile_fx_inner`: </span>
    <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">    This function is used to save arguments for a compile_fx_inner function call
</span><span style="color:#b44">    to the file system.  Later on one can replay the compile_fx_inner call
</span><span style="color:#b44">    with the saved arguments using load_args_and_run_compile_fx_inner.
</span><span style="color:#b44">    &#34;&#34;&#34;</span>
    <span style="color:#a2f;font-weight:bold">if</span> config<span style="color:#666">.</span>save_args:
        save_args_for_compile_fx_inner(
            gm,
            example_inputs,
            cudagraphs<span style="color:#666">=</span>cudagraphs,
            num_fixed<span style="color:#666">=</span>num_fixed,
            is_backward<span style="color:#666">=</span>is_backward,
            graph_id<span style="color:#666">=</span>graph_id,
            cpp_wrapper<span style="color:#666">=</span>cpp_wrapper,
            aot_mode<span style="color:#666">=</span>aot_mode,
            is_inference<span style="color:#666">=</span>is_inference,
            boxed_forward_device_index<span style="color:#666">=</span>boxed_forward_device_index,
            user_visible_outputs<span style="color:#666">=</span>user_visible_outputs,
            layout_opt<span style="color:#666">=</span>layout_opt,
        )
</code></pre></div><ul>
<li>
<h3 id="调用fx_codegen_and_compile进行计算图编译">调用fx_codegen_and_compile进行计算图编译</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">if</span> cudagraphs <span style="color:#a2f;font-weight:bold">is</span> None:
        cudagraphs <span style="color:#666">=</span> BoxedBool(config<span style="color:#666">.</span>triton<span style="color:#666">.</span>cudagraphs)

    <span style="color:#080;font-style:italic"># Inputs to fx_codegen_and_compile</span>
    graph_args <span style="color:#666">=</span> [gm, example_inputs]
    graph_kwargs <span style="color:#666">=</span> {
        <span style="color:#b44">&#34;cudagraphs&#34;</span>: cudagraphs,
        <span style="color:#b44">&#34;num_fixed&#34;</span>: num_fixed,
        <span style="color:#b44">&#34;is_backward&#34;</span>: is_backward,
        <span style="color:#b44">&#34;graph_id&#34;</span>: graph_id,
        <span style="color:#b44">&#34;cpp_wrapper&#34;</span>: cpp_wrapper,
        <span style="color:#b44">&#34;aot_mode&#34;</span>: aot_mode,
        <span style="color:#b44">&#34;is_inference&#34;</span>: is_inference,
        <span style="color:#b44">&#34;user_visible_outputs&#34;</span>: user_visible_outputs,
        <span style="color:#b44">&#34;layout_opt&#34;</span>: layout_opt,
    }
        
    compiled_graph: CompiledFxGraph <span style="color:#666">=</span> fx_codegen_and_compile(
        <span style="color:#666">*</span>graph_args, <span style="color:#666">**</span>graph_kwargs  <span style="color:#080;font-style:italic"># type: ignore[arg-type]</span>
    )
</code></pre></div><p>后处理（忽略cudagraph相关的逻辑分支）</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#080;font-style:italic"># NOTE. post-process. 调用`align_inputs`处理inputs的内存对齐，对non-fixed inputs进行内存对齐检查，没对齐的话则调用`copy_misaligned_inputs`进行内存对齐处理（实际是用as_strided方法实现）。 </span>
    <span style="color:#080;font-style:italic"># cudagraphs does its own aligning of inputs</span>
    <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f;font-weight:bold">not</span> cudagraphs:
        new_callable <span style="color:#666">=</span> align_inputs(
            compiled_graph<span style="color:#666">.</span>get_current_callable(), example_inputs, <span style="color:#a2f">range</span>(num_fixed)
        )
        <span style="color:#a2f;font-weight:bold">if</span> new_callable <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> compiled_graph<span style="color:#666">.</span>get_current_callable():
            compiled_graph<span style="color:#666">.</span>current_callable <span style="color:#666">=</span> new_callable

    <span style="color:#080;font-style:italic"># aot autograd needs to know to pass in inputs as a list</span>
    compiled_graph<span style="color:#666">.</span>_boxed_call <span style="color:#666">=</span> True
    <span style="color:#a2f;font-weight:bold">return</span> compiled_graph
</code></pre></div><ul>
<li>
<h3 id="fx_codegen_and_compile">fx_codegen_and_compile</h3>
</li>
</ul>
<p>做一些autograd的后处理，然后用GraphLowering进行fx graph的lowering（GraphLowering.run）和codegen（Graphlowerin.compile_to_fn），生成CompiledFxGraph</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#080;font-style:italic"># NOTE. 遍历node，将aten.view替换成aten.reshape</span>
    view_to_reshape(gm)
    fake_mode <span style="color:#666">=</span> fake_tensor_prop(gm, example_inputs)

    <span style="color:#080;font-style:italic"># NOTE. 处理`functionalized`后的graph，无效代码消除等.</span>
    <span style="color:#a2f;font-weight:bold">with</span> V<span style="color:#666">.</span>set_fake_mode(fake_mode):  <span style="color:#080;font-style:italic"># type: ignore[call-arg]</span>
        <span style="color:#080;font-style:italic"># has some issues with memory in training</span>
        post_grad_passes(gm, is_inference<span style="color:#666">=</span>is_inference)
        V<span style="color:#666">.</span>debug<span style="color:#666">.</span>fx_graph_transformed(gm, example_inputs)

    <span style="color:#a2f;font-weight:bold">with</span> V<span style="color:#666">.</span>set_fake_mode(fake_mode):  <span style="color:#080;font-style:italic"># type: ignore[call-arg]</span>
        graph <span style="color:#666">=</span> GraphLowering(
            gm,
            shape_env<span style="color:#666">=</span>shape_env,
            num_static_inputs<span style="color:#666">=</span>num_fixed,
            graph_id<span style="color:#666">=</span>graph_id,
            cpp_wrapper<span style="color:#666">=</span>cpp_wrapper,
            aot_mode<span style="color:#666">=</span>aot_mode,
            user_visible_outputs<span style="color:#666">=</span>user_visible_outputs,
        )
        <span style="color:#a2f;font-weight:bold">with</span> V<span style="color:#666">.</span>set_graph_handler(graph):  <span style="color:#080;font-style:italic"># type: ignore[call-arg]</span>
            graph<span style="color:#666">.</span>run(<span style="color:#666">*</span>example_inputs)
            compiled_fn <span style="color:#666">=</span> graph<span style="color:#666">.</span>compile_to_fn()

            compiled_graph <span style="color:#666">=</span> CompiledFxGraph(
                compiled_artifact<span style="color:#666">=</span>compiled_fn,
                cache_key<span style="color:#666">=</span>graph<span style="color:#666">.</span>cache_key,
                artifact_path<span style="color:#666">=</span>graph<span style="color:#666">.</span>cache_path,
                cache_linemap<span style="color:#666">=</span>graph<span style="color:#666">.</span>cache_linemap,
                device_types<span style="color:#666">=</span>graph<span style="color:#666">.</span>device_types,
                device_idxs<span style="color:#666">=</span>graph<span style="color:#666">.</span>device_idxs,
                mutated_inputs<span style="color:#666">=</span>graph<span style="color:#666">.</span>mutated_inputs,
                mutated_input_idxs<span style="color:#666">=</span><span style="color:#a2f">set</span>(graph<span style="color:#666">.</span>mutated_input_idxs),
            )
    <span style="color:#a2f;font-weight:bold">return</span> compiled_graph
</code></pre></div><h1 id="graphlowering">GraphLowering</h1>
<ul>
<li>
<h2 id="interpreter">Interpreter</h2>
</li>
<li>
<h3 id="声明和注释">声明和注释</h3>
</li>
</ul>
<p>按拓扑序对node进行解释执行。call_function和call_method的区别是：(1) <code>call_function</code>一般是<code>torch.aten.mul</code>这类op (2) <code>call_method</code>一般是 <code>Tensor.add</code> 这类method。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f">@compatibility</span>(is_backward_compatible<span style="color:#666">=</span>True)
<span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">Interpreter</span>:
    <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">    An Interpreter executes an FX graph Node-by-Node. This pattern
</span><span style="color:#b44">    can be useful for many things, including writing code
</span><span style="color:#b44">    transformations as well as analysis passes.
</span><span style="color:#b44">
</span><span style="color:#b44">    Methods in the Interpreter class can be overridden to customize
</span><span style="color:#b44">    the behavior of execution. The map of overrideable methods
</span><span style="color:#b44">    in terms of call hierarchy::
</span><span style="color:#b44">
</span><span style="color:#b44">        run()
</span><span style="color:#b44">            +-- run_node
</span><span style="color:#b44">                +-- placeholder()
</span><span style="color:#b44">                +-- get_attr()
</span><span style="color:#b44">                +-- call_function()
</span><span style="color:#b44">                +-- call_method()
</span><span style="color:#b44">                +-- call_module()
</span><span style="color:#b44">                +-- output()
</span><span style="color:#b44">     &#34;&#34;&#34;</span>
</code></pre></div><ul>
<li>
<h3 id="run核心逻辑">run核心逻辑</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f">@compatibility</span>(is_backward_compatible<span style="color:#666">=</span>True)
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">run</span>(self, <span style="color:#666">*</span>args, initial_env : Optional[Dict[Node, Any]] <span style="color:#666">=</span> None, enable_io_processing : <span style="color:#a2f">bool</span> <span style="color:#666">=</span> True) <span style="color:#666">-&gt;</span> Any:
        <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">        Run `module` via interpretation and return the result.
</span><span style="color:#b44">        &#34;&#34;&#34;</span>
        self<span style="color:#666">.</span>env <span style="color:#666">=</span> initial_env <span style="color:#a2f;font-weight:bold">if</span> initial_env <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> None <span style="color:#a2f;font-weight:bold">else</span> {}
        <span style="color:#a2f;font-weight:bold">for</span> node <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>module<span style="color:#666">.</span>graph<span style="color:#666">.</span>nodes:
            <span style="color:#080;font-style:italic"># NOTE. 先检查当前node是否被执行过，应该是考虑到图中有环</span>
            <span style="color:#a2f;font-weight:bold">if</span> node <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>env:
                <span style="color:#a2f;font-weight:bold">continue</span>
            <span style="color:#080;font-style:italic"># NOTE. 调用 run_node 执行这个node，并将结果保存在env中</span>
            <span style="color:#080;font-style:italic"># NOTE. 调用 run_node 的方式是 self.run_node，类似 c++ 中 this指针，所以派生类的 run 会动态的调用到派生类的 run_node 的实现上。</span>
            <span style="color:#a2f;font-weight:bold">try</span>:
                self<span style="color:#666">.</span>env[node] <span style="color:#666">=</span> self<span style="color:#666">.</span>run_node(node)
            <span style="color:#a2f;font-weight:bold">except</span> <span style="color:#d2413a;font-weight:bold">Exception</span> <span style="color:#a2f;font-weight:bold">as</span> e:
                <span style="color:#a2f;font-weight:bold">raise</span>
            <span style="color:#a2f;font-weight:bold">if</span> self<span style="color:#666">.</span>garbage_collect_values:
                <span style="color:#a2f;font-weight:bold">for</span> to_delete <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>user_to_last_uses<span style="color:#666">.</span>get(node, []):
                    <span style="color:#a2f;font-weight:bold">del</span> self<span style="color:#666">.</span>env[to_delete]
            <span style="color:#080;font-style:italic"># NOTE. 如果是output 节点，说明图已经执行完了，返回整个图的执行结果</span>
            <span style="color:#a2f;font-weight:bold">if</span> node<span style="color:#666">.</span>op <span style="color:#666">==</span> <span style="color:#b44">&#39;output&#39;</span>:
                output_val <span style="color:#666">=</span> self<span style="color:#666">.</span>env[node]
                <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>module<span style="color:#666">.</span>graph<span style="color:#666">.</span>process_outputs(output_val) <span style="color:#a2f;font-weight:bold">if</span> enable_io_processing <span style="color:#a2f;font-weight:bold">else</span> output_val
</code></pre></div><ul>
<li>
<h3 id="run_node">run_node</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f">@compatibility</span>(is_backward_compatible<span style="color:#666">=</span>True)
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">run_node</span>(self, n : Node) <span style="color:#666">-&gt;</span> Any:
        <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">        Run a specific node ``n`` and return the result.
</span><span style="color:#b44">        Calls into placeholder, get_attr, call_function,
</span><span style="color:#b44">        call_method, call_module, or output depending
</span><span style="color:#b44">        on ``node.op``
</span><span style="color:#b44">
</span><span style="color:#b44">        Args:
</span><span style="color:#b44">            n (Node): The Node to execute
</span><span style="color:#b44">
</span><span style="color:#b44">        Returns:
</span><span style="color:#b44">            Any: The result of executing ``n``
</span><span style="color:#b44">        &#34;&#34;&#34;</span>
        <span style="color:#080;font-style:italic"># NOTE. 保存当前node的meta信息到global的上下文（一个map）</span>
        <span style="color:#a2f;font-weight:bold">with</span> self<span style="color:#666">.</span>_set_current_node(n):
            <span style="color:#080;font-style:italic"># NOTE. 从 `env` 中获取当前node的操作数，函数`run`会将每个节点的执行结果保存在env，所以当前节点依赖的操作数应该可以从 `env`中获取。</span>
            args, kwargs <span style="color:#666">=</span> self<span style="color:#666">.</span>fetch_args_kwargs_from_env(n)
            <span style="color:#a2f;font-weight:bold">assert</span> <span style="color:#a2f">isinstance</span>(args, <span style="color:#a2f">tuple</span>)
            <span style="color:#a2f;font-weight:bold">assert</span> <span style="color:#a2f">isinstance</span>(kwargs, <span style="color:#a2f">dict</span>)
            <span style="color:#080;font-style:italic"># NOTE. 根据当前节点的op类型选择对应的执行函数，如 call_function, call_method...</span>
            <span style="color:#a2f;font-weight:bold">return</span> <span style="color:#a2f">getattr</span>(self, n<span style="color:#666">.</span>op)(n<span style="color:#666">.</span>target, args, kwargs)
</code></pre></div><ul>
<li>
<h3 id="call_function">call_function</h3>
</li>
</ul>
<p>默认的实现是直接调用这个对应的函数实例target</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">call_function</span>(self, target : <span style="color:#b44">&#39;Target&#39;</span>, args : Tuple[Argument, <span style="color:#666">...</span>], kwargs : Dict[<span style="color:#a2f">str</span>, Any]) <span style="color:#666">-&gt;</span> Any:
        <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">        Execute a ``call_function`` node and return the result.
</span><span style="color:#b44">        &#34;&#34;&#34;</span>
        <span style="color:#a2f;font-weight:bold">assert</span> <span style="color:#a2f;font-weight:bold">not</span> <span style="color:#a2f">isinstance</span>(target, <span style="color:#a2f">str</span>)
        <span style="color:#080;font-style:italic"># Execute the function and return the result</span>
        <span style="color:#a2f;font-weight:bold">return</span> target(<span style="color:#666">*</span>args, <span style="color:#666">**</span>kwargs)
</code></pre></div><ul>
<li>
<h2 id="graphlowering-1">GraphLowering</h2>
</li>
<li>
<h3 id="run_node核心逻辑">run_node核心逻辑</h3>
</li>
</ul>
<p><code>call_finction</code>仅仅是实现将 op lowering到 Inductor IR，后处理还需要在合适的node进行realize。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">run_node</span>(self, n: torch<span style="color:#666">.</span>fx<span style="color:#666">.</span>Node):
        origins <span style="color:#666">=</span> {n}
        <span style="color:#080;font-style:italic"># NOTE. 经过dynamo和functionlization之后，graph中都是call_function节点</span>
        <span style="color:#a2f;font-weight:bold">if</span> n<span style="color:#666">.</span>op <span style="color:#666">==</span> <span style="color:#b44">&#34;call_function&#34;</span>:
            args, kwargs <span style="color:#666">=</span> self<span style="color:#666">.</span>fetch_args_kwargs_from_env(n)
            origins <span style="color:#666">|=</span> gather_origins(args, kwargs)
        <span style="color:#a2f;font-weight:bold">with</span> ir<span style="color:#666">.</span>IRNode<span style="color:#666">.</span>current_origins(origins), self<span style="color:#666">.</span>set_current_node(n):
            <span style="color:#666">...</span>
            <span style="color:#a2f;font-weight:bold">elif</span> n<span style="color:#666">.</span>op <span style="color:#666">==</span> <span style="color:#b44">&#34;call_function&#34;</span> <span style="color:#a2f;font-weight:bold">and</span> n<span style="color:#666">.</span>target <span style="color:#a2f;font-weight:bold">in</span> layout_constraints:
                <span style="color:#080;font-style:italic"># NOTE. 调用 call_function 函数进行lowering and codegen</span>
                args, kwargs <span style="color:#666">=</span> layout_constraints[n<span style="color:#666">.</span>target](n, <span style="color:#666">*</span>args, <span style="color:#666">**</span>kwargs)
                result <span style="color:#666">=</span> self<span style="color:#666">.</span>call_function(n<span style="color:#666">.</span>target, args, kwargs)
            <span style="color:#666">...</span>
            <span style="color:#a2f;font-weight:bold">else</span>:
                result <span style="color:#666">=</span> <span style="color:#a2f">super</span>()<span style="color:#666">.</span>run_node(n)

            <span style="color:#080;font-style:italic"># NOTE. 后处理 outputs中的stride,offset，使result保持stride order：`result = ir.ExternKernel.require_stride_order(result, stride_order)`</span>
            <span style="color:#080;font-style:italic"># require the same stride order for dense outputs,</span>
            <span style="color:#080;font-style:italic"># 1. user-land view() will not throw because inductor</span>
            <span style="color:#080;font-style:italic"># output different strides than eager</span>
            <span style="color:#080;font-style:italic"># long term the solution is to make view() always succeed</span>
            <span style="color:#080;font-style:italic"># with infallible strides.</span>
            <span style="color:#080;font-style:italic"># 2: as_strided ops, we need make sure its input has same size/stride with</span>
            <span style="color:#080;font-style:italic"># eager model to align with eager behavior.</span>
            as_strided_ops <span style="color:#666">=</span> [
                torch<span style="color:#666">.</span>ops<span style="color:#666">.</span>aten<span style="color:#666">.</span>as_strided<span style="color:#666">.</span>default,
                torch<span style="color:#666">.</span>ops<span style="color:#666">.</span>aten<span style="color:#666">.</span>as_strided_<span style="color:#666">.</span>default,
                torch<span style="color:#666">.</span>ops<span style="color:#666">.</span>aten<span style="color:#666">.</span>as_strided_scatter<span style="color:#666">.</span>default,
            ]
            is_output <span style="color:#666">=</span> <span style="color:#a2f">any</span>(user<span style="color:#666">.</span>op <span style="color:#666">==</span> <span style="color:#b44">&#34;output&#34;</span> <span style="color:#a2f;font-weight:bold">for</span> user <span style="color:#a2f;font-weight:bold">in</span> n<span style="color:#666">.</span>users)
            is_input_for_as_strided <span style="color:#666">=</span> <span style="color:#a2f">any</span>(
                user<span style="color:#666">.</span>target <span style="color:#a2f;font-weight:bold">in</span> as_strided_ops <span style="color:#a2f;font-weight:bold">for</span> user <span style="color:#a2f;font-weight:bold">in</span> n<span style="color:#666">.</span>users
            )
            <span style="color:#a2f;font-weight:bold">if</span> (is_output <span style="color:#a2f;font-weight:bold">or</span> is_input_for_as_strided) <span style="color:#a2f;font-weight:bold">and</span> <span style="color:#a2f">isinstance</span>(
                n<span style="color:#666">.</span>meta[<span style="color:#b44">&#34;val&#34;</span>], torch<span style="color:#666">.</span>Tensor
            ):
                strides <span style="color:#666">=</span> n<span style="color:#666">.</span>meta[<span style="color:#b44">&#34;val&#34;</span>]<span style="color:#666">.</span>stride()
                dense <span style="color:#666">=</span> torch<span style="color:#666">.</span>_prims_common<span style="color:#666">.</span>is_non_overlapping_and_dense(n<span style="color:#666">.</span>meta[<span style="color:#b44">&#34;val&#34;</span>])
                <span style="color:#080;font-style:italic"># requiring a stride order for a non-dense output wouldn&#39;t</span>
                <span style="color:#080;font-style:italic"># recreate the same strides, and would fail with view, defer for now.</span>
                <span style="color:#a2f;font-weight:bold">if</span> dense <span style="color:#a2f;font-weight:bold">and</span> <span style="color:#a2f">len</span>(strides):
                    stride_order <span style="color:#666">=</span> ir<span style="color:#666">.</span>get_stride_order(strides)
                    <span style="color:#a2f;font-weight:bold">if</span> (
                        <span style="color:#a2f">len</span>(result<span style="color:#666">.</span>get_size()) <span style="color:#666">==</span> <span style="color:#666">4</span>
                        <span style="color:#a2f;font-weight:bold">and</span> n <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>nodes_prefer_channels_last
                        <span style="color:#a2f;font-weight:bold">and</span> n<span style="color:#666">.</span>name <span style="color:#a2f;font-weight:bold">not</span> <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>user_visible_outputs
                        <span style="color:#a2f;font-weight:bold">and</span> <span style="color:#a2f;font-weight:bold">not</span> is_input_for_as_strided
                    ):
                        stride_order <span style="color:#666">=</span> ir<span style="color:#666">.</span>NHWC_STRIDE_ORDER
                    result <span style="color:#666">=</span> ir<span style="color:#666">.</span>ExternKernel<span style="color:#666">.</span>require_stride_order(result, stride_order)

            <span style="color:#080;font-style:italic"># NOTE. 后处理，判断是否对节点进行实例化（call result.realize_hint）</span>
            <span style="color:#080;font-style:italic"># NOTE. 选出了一些需要realize operand的op（needs_realized_inputs），检查当前节点的users中是否需要realize operand，是的话则进行realize处理。</span>
            <span style="color:#080;font-style:italic"># Realize if (1) any user need inputs realized, or (2) there is</span>
            <span style="color:#080;font-style:italic"># already too many reads and rematerializing can be bad.</span>
            num_users <span style="color:#666">=</span> <span style="color:#a2f">len</span>(<span style="color:#a2f">set</span>(n<span style="color:#666">.</span>users))
            <span style="color:#a2f;font-weight:bold">if</span> num_users <span style="color:#666">&gt;</span> <span style="color:#666">1</span> <span style="color:#a2f;font-weight:bold">and</span> <span style="color:#a2f">isinstance</span>(result, TensorBox):
                <span style="color:#a2f;font-weight:bold">for</span> user <span style="color:#a2f;font-weight:bold">in</span> n<span style="color:#666">.</span>users:
                    <span style="color:#a2f;font-weight:bold">if</span> user<span style="color:#666">.</span>target <span style="color:#a2f;font-weight:bold">in</span> needs_realized_inputs:
                        <span style="color:#080;font-style:italic"># NOTE. 如果满足条件，最终会调用 realize()</span>
                        result<span style="color:#666">.</span>realize_hint()
                        <span style="color:#666">...</span>
                        <span style="color:#a2f;font-weight:bold">if</span> user<span style="color:#666">.</span>target <span style="color:#a2f;font-weight:bold">in</span> need_fixed_layout:
                            result <span style="color:#666">=</span> ir<span style="color:#666">.</span>ExternKernel<span style="color:#666">.</span>require_stride_order(
                                result, ir<span style="color:#666">.</span>get_stride_order(n<span style="color:#666">.</span>meta[<span style="color:#b44">&#34;val&#34;</span>]<span style="color:#666">.</span>stride())
                            )
                    <span style="color:#080;font-style:italic"># NOTE. 如果当前node结果是output，那么需要实例化。</span>
                    <span style="color:#a2f;font-weight:bold">if</span> user<span style="color:#666">.</span>op <span style="color:#666">==</span> <span style="color:#b44">&#34;output&#34;</span>:
                        <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(result<span style="color:#666">.</span>data<span style="color:#666">.</span>data, (Pointwise, Reduction)):
                            result<span style="color:#666">.</span>realize()
                result<span style="color:#666">.</span>mark_reuse(<span style="color:#a2f">len</span>(n<span style="color:#666">.</span>users))

            <span style="color:#080;font-style:italic"># NOTE. 如果当前node节点read数量大于阈值（典型值=8）则尝试实例化node，防止大量的重复计算。</span>
            <span style="color:#080;font-style:italic"># Realize if the IRNode already has accumulated lots of reads</span>
            <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(result, TensorBox) <span style="color:#a2f;font-weight:bold">and</span> result<span style="color:#666">.</span>has_exceeded_max_reads():
                <span style="color:#080;font-style:italic"># Prevent excessive accumulation in a computed buffer, when</span>
                <span style="color:#080;font-style:italic"># there are multiple branches each with small number of memory</span>
                <span style="color:#080;font-style:italic"># reads, but they converge to a user.</span>
                result<span style="color:#666">.</span>realize_hint()
        <span style="color:#666">...</span>
        self<span style="color:#666">.</span>register_users_of(result)

        <span style="color:#a2f;font-weight:bold">return</span> result
</code></pre></div><ul>
<li>
<h3 id="call_function-1">call_function</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">call_function</span>(self, target, args, kwargs):
        <span style="color:#080;font-style:italic"># NOTE. 处理 getitem，直接走eager的逻辑</span>
        <span style="color:#a2f;font-weight:bold">if</span> target <span style="color:#a2f;font-weight:bold">is</span> operator<span style="color:#666">.</span>getitem <span style="color:#a2f;font-weight:bold">and</span> <span style="color:#a2f">isinstance</span>(args[<span style="color:#666">0</span>], (<span style="color:#a2f">list</span>, <span style="color:#a2f">tuple</span>)):
            <span style="color:#a2f;font-weight:bold">return</span> <span style="color:#a2f">super</span>()<span style="color:#666">.</span>call_function(target, args, kwargs)

        <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">hasattr</span>(target, <span style="color:#b44">&#34;_inductor_lowering_function&#34;</span>):
            <span style="color:#080;font-style:italic"># passthrough lowerings from .pattern_matcher</span>
            <span style="color:#a2f;font-weight:bold">return</span> target(<span style="color:#666">*</span>args, <span style="color:#666">**</span>kwargs)

        <span style="color:#080;font-style:italic"># NOTE. lowerings中没有注册对应target的lowering实现，如果允许fallback则走eager，否则抛出异常</span>
        <span style="color:#a2f;font-weight:bold">if</span> target <span style="color:#a2f;font-weight:bold">not</span> <span style="color:#a2f;font-weight:bold">in</span> lowerings:
            <span style="color:#666">...</span>
        <span style="color:#a2f;font-weight:bold">try</span>:
            out <span style="color:#666">=</span> lowerings[target](<span style="color:#666">*</span>args, <span style="color:#666">**</span>kwargs)
            <span style="color:#a2f;font-weight:bold">return</span> out
        <span style="color:#a2f;font-weight:bold">except</span> <span style="color:#d2413a;font-weight:bold">Exception</span> <span style="color:#a2f;font-weight:bold">as</span> e:
            <span style="color:#666">...</span>
</code></pre></div><ul>
<li>
<h3 id="compile_to_fn">compile_to_fn</h3>
</li>
</ul>
<p><code>compile_fx</code>中会调用<code>compile_to_fn</code>对Inductor IR进行codegen。<code>compile_to_fn</code>调用<code>codegen</code>函数进行codegen。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">codegen</span>(self):
        <span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">.scheduler</span> <span style="color:#a2f;font-weight:bold">import</span> Scheduler

        self<span style="color:#666">.</span>init_wrapper_code()

        self<span style="color:#666">.</span>scheduler <span style="color:#666">=</span> Scheduler(self<span style="color:#666">.</span>buffers)
        <span style="color:#a2f;font-weight:bold">assert</span> self<span style="color:#666">.</span>scheduler <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> None  <span style="color:#080;font-style:italic"># mypy can&#39;t figure this out</span>
        <span style="color:#080;font-style:italic"># NOTE. 生成kernel</span>
        self<span style="color:#666">.</span>scheduler<span style="color:#666">.</span>codegen()
        <span style="color:#a2f;font-weight:bold">assert</span> self<span style="color:#666">.</span>wrapper_code <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> None
        <span style="color:#080;font-style:italic"># # NOTE. 生成wrapper</span>
        <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>wrapper_code<span style="color:#666">.</span>generate()
</code></pre></div><p>buffers则是lowering的结果（Inductor IR），如下函数forward</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(x, y):
    x <span style="color:#666">=</span> torch<span style="color:#666">.</span>ops<span style="color:#666">.</span>aten<span style="color:#666">.</span>sin(x)
    z0 <span style="color:#666">=</span> torch<span style="color:#666">.</span>ops<span style="color:#666">.</span>aten<span style="color:#666">.</span>mm(x, y)
    z1 <span style="color:#666">=</span> torch<span style="color:#666">.</span>ops<span style="color:#666">.</span>aten<span style="color:#666">.</span>cos(z0)
    z2 <span style="color:#666">=</span> z0 <span style="color:#666">+</span> z1
    <span style="color:#a2f;font-weight:bold">return</span> z2
</code></pre></div><p>经过lowering得到的ComputedBuffer如下：
计算图被拆分成三个stage（此时还未经过fusion，所以不一定是三个kernel，但是这个case中间的stage是mm，基本是确定要调库的，所以最终生成三个kernel），第一个stage计算sin(x)，其result会realize到buf0（ComputedBuffer）中；mm对应的是第二个stage ExternKernelOut，其inputs包含<code>buf0</code>和<code>arg1_1</code>，后者是一个<code>InputBuffer</code>，会realize出<code>buf1</code>；最后一个stage计算 cos和add，realize出<code>buf2</code>。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">[ComputedBuffer(name<span style="color:#666">=</span><span style="color:#b44">&#39;buf0&#39;</span>, layout<span style="color:#666">=</span>FixedLayout(<span style="color:#b44">&#39;cuda&#39;</span>, torch<span style="color:#666">.</span>float32, size<span style="color:#666">=</span>[<span style="color:#666">3</span>, <span style="color:#666">4</span>], stride<span style="color:#666">=</span>[<span style="color:#666">4</span>, <span style="color:#666">1</span>]), data<span style="color:#666">=</span>Pointwise(
  <span style="color:#b44">&#39;cuda&#39;</span>,
  torch<span style="color:#666">.</span>float32,
  <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">inner_fn</span>(index):
      i0, i1 <span style="color:#666">=</span> index
      tmp0 <span style="color:#666">=</span> ops<span style="color:#666">.</span>load(arg0_1, i1 <span style="color:#666">+</span> <span style="color:#666">4</span> <span style="color:#666">*</span> i0)
      tmp1 <span style="color:#666">=</span> ops<span style="color:#666">.</span>sin(tmp0)
      <span style="color:#a2f;font-weight:bold">return</span> tmp1
  ,
  ranges<span style="color:#666">=</span>[<span style="color:#666">3</span>, <span style="color:#666">4</span>],
  origin_node<span style="color:#666">=</span>sin,
  origins<span style="color:#666">=</span>{sin}
)), ExternKernelOut(name<span style="color:#666">=</span><span style="color:#b44">&#39;buf1&#39;</span>, layout<span style="color:#666">=</span>FixedLayout(<span style="color:#b44">&#39;cuda&#39;</span>, torch<span style="color:#666">.</span>float32, size<span style="color:#666">=</span>[<span style="color:#666">3</span>, <span style="color:#666">6</span>], stride<span style="color:#666">=</span>[<span style="color:#666">6</span>, <span style="color:#666">1</span>]), inputs<span style="color:#666">=</span>[ComputedBuffer(name<span style="color:#666">=</span><span style="color:#b44">&#39;buf0&#39;</span>, layout<span style="color:#666">=</span>FixedLayout(<span style="color:#b44">&#39;cuda&#39;</span>, torch<span style="color:#666">.</span>float32, size<span style="color:#666">=</span>[<span style="color:#666">3</span>, <span style="color:#666">4</span>], stride<span style="color:#666">=</span>[<span style="color:#666">4</span>, <span style="color:#666">1</span>]), data<span style="color:#666">=</span>Pointwise(
  <span style="color:#b44">&#39;cuda&#39;</span>,
  torch<span style="color:#666">.</span>float32,
  <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">inner_fn</span>(index):
      i0, i1 <span style="color:#666">=</span> index
      tmp0 <span style="color:#666">=</span> ops<span style="color:#666">.</span>load(arg0_1, i1 <span style="color:#666">+</span> <span style="color:#666">4</span> <span style="color:#666">*</span> i0)
      tmp1 <span style="color:#666">=</span> ops<span style="color:#666">.</span>sin(tmp0)
      <span style="color:#a2f;font-weight:bold">return</span> tmp1
  ,
  ranges<span style="color:#666">=</span>[<span style="color:#666">3</span>, <span style="color:#666">4</span>],
  origin_node<span style="color:#666">=</span>sin,
  origins<span style="color:#666">=</span>{sin}
)), InputBuffer(name<span style="color:#666">=</span><span style="color:#b44">&#39;arg1_1&#39;</span>, layout<span style="color:#666">=</span>FixedLayout(<span style="color:#b44">&#39;cuda&#39;</span>, torch<span style="color:#666">.</span>float32, size<span style="color:#666">=</span>[<span style="color:#666">4</span>, <span style="color:#666">6</span>], stride<span style="color:#666">=</span>[<span style="color:#666">6</span>, <span style="color:#666">1</span>]))], constant_args<span style="color:#666">=</span>(), kwargs<span style="color:#666">=</span>{}, output_view<span style="color:#666">=</span>None), ComputedBuffer(name<span style="color:#666">=</span><span style="color:#b44">&#39;buf2&#39;</span>, layout<span style="color:#666">=</span>FixedLayout(<span style="color:#b44">&#39;cuda&#39;</span>, torch<span style="color:#666">.</span>float32, size<span style="color:#666">=</span>[<span style="color:#666">3</span>, <span style="color:#666">6</span>], stride<span style="color:#666">=</span>[<span style="color:#666">6</span>, <span style="color:#666">1</span>]), data<span style="color:#666">=</span>Pointwise(
  <span style="color:#b44">&#39;cuda&#39;</span>,
  torch<span style="color:#666">.</span>float32,
  <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">inner_fn</span>(index):
      i0, i1 <span style="color:#666">=</span> index
      tmp0 <span style="color:#666">=</span> ops<span style="color:#666">.</span>load(buf1, i1 <span style="color:#666">+</span> <span style="color:#666">6</span> <span style="color:#666">*</span> i0)
      tmp1 <span style="color:#666">=</span> ops<span style="color:#666">.</span>load(buf1, i1 <span style="color:#666">+</span> <span style="color:#666">6</span> <span style="color:#666">*</span> i0)
      tmp2 <span style="color:#666">=</span> ops<span style="color:#666">.</span>cos(tmp1)
      tmp3 <span style="color:#666">=</span> tmp0 <span style="color:#666">+</span> tmp2
      <span style="color:#a2f;font-weight:bold">return</span> tmp3
  ,
  ranges<span style="color:#666">=</span>[<span style="color:#666">3</span>, <span style="color:#666">6</span>],
  origin_node<span style="color:#666">=</span>add,
  origins<span style="color:#666">=</span>{cos, add}
))]
</code></pre></div><p>如果需要<strong>从Inductor接入后端</strong>，需要从_inductor/codegen中注册，参考_inductor/codegen/common.py:</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic"># The code generated by Inductor consists of two main parts: kernel code and wrapper code.</span>
<span style="color:#080;font-style:italic"># For any new backend looking to integrate with Inductor, customization of these two main</span>
<span style="color:#080;font-style:italic"># parts are necessary to generate its specific code.</span>
<span style="color:#080;font-style:italic">#</span>
<span style="color:#080;font-style:italic"># Kernel code generation is determined by different Scheduling. Consequently, a new</span>
<span style="color:#080;font-style:italic"># backend needs to provide a custom Scheduling for its unique kernel code generation. Currently,</span>
<span style="color:#080;font-style:italic"># CppScheduling and TritonScheduling serve the C++/OpenMP and Triton backends, respectively.</span>
<span style="color:#080;font-style:italic">#</span>
<span style="color:#080;font-style:italic"># For the Wrapper, Inductor provides a WrapperCodeGen class to generate the Python wrapper code</span>
<span style="color:#080;font-style:italic"># that bridges kernels. This allows out-of-tree backends to inherit from WrapperCodeGen,</span>
<span style="color:#080;font-style:italic"># and override specific member functions to create backend-specific Python wrapper code.</span>
<span style="color:#080;font-style:italic">#</span>
<span style="color:#080;font-style:italic"># Other classes, such as CppKernel and TritonKernel, used for code generation, typically form part</span>
<span style="color:#080;font-style:italic"># of the logic for either Scheduling or WrapperCodeGen. So the Scheduling and WrapperCodeGen interfaces</span>
<span style="color:#080;font-style:italic"># provide flexibility to the backend. A backend can choose to implement these classes from scratch,</span>
<span style="color:#080;font-style:italic"># or reuse them by extending and overriding as necessary. And Inductor provides the registration API,</span>
<span style="color:#080;font-style:italic"># register_backend_for_device, to equip a new backend at runtime.</span>
<span style="color:#080;font-style:italic">#</span>
<span style="color:#080;font-style:italic"># Intel has developed a new backend on top of Triton to support Intel GPUs, leveraging these interfaces.</span>
<span style="color:#080;font-style:italic"># This backend can be used as a reference:</span>
<span style="color:#080;font-style:italic"># https://github.com/intel/intel-extension-for-pytorch/blob/5dcc9d57e5422cf295e1a1ee97896d6b6a554a85/intel_extension_for_pytorch/_inductor/__init__.py#L9</span>
</code></pre></div><h1 id="scheduler">Scheduler</h1>
<ul>
<li>
<h2 id="__init__"><strong>init</strong></h2>
</li>
</ul>
<p>一些重要的逻辑实现在初始化阶段。<code>__init__(self, nodes)</code> 中的nodes即buffers，即Inductor IR的Buffer数组。这个函数（1）将ir.Buffer转换成ScheduerNode（2）node间依赖分析（3）以node为粒度进行fusion</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> __init__(self, nodes):
        <span style="color:#a2f">super</span>()<span style="color:#666">.</span>__init__()
        self<span style="color:#666">.</span>backends <span style="color:#666">=</span> {}
        self<span style="color:#666">.</span>fuse_cache <span style="color:#666">=</span> {}

        self<span style="color:#666">.</span>nodes <span style="color:#666">=</span> []
        self<span style="color:#666">.</span>available_buffer_names <span style="color:#666">=</span> {
            <span style="color:#666">*</span>V<span style="color:#666">.</span>graph<span style="color:#666">.</span>graph_inputs<span style="color:#666">.</span>keys(),
            <span style="color:#666">*</span>V<span style="color:#666">.</span>graph<span style="color:#666">.</span>constants<span style="color:#666">.</span>keys(),
        }

        <span style="color:#080;font-style:italic"># NOTE. 将 no-op转换为`NopKernelSchedulerNode`，`ExterKernel`转为`ExternKernelSchedulerNode`，`ComputedBuffer`或`TemplateBuffer`转为`SchedulerNode`</span>
        self<span style="color:#666">.</span>nodes <span style="color:#666">=</span> [self<span style="color:#666">.</span>create_scheduler_node(n) <span style="color:#a2f;font-weight:bold">for</span> n <span style="color:#a2f;font-weight:bold">in</span> nodes]

        <span style="color:#080;font-style:italic"># some new constants could have been created above</span>
        self<span style="color:#666">.</span>available_buffer_names<span style="color:#666">.</span>update(V<span style="color:#666">.</span>graph<span style="color:#666">.</span>constants<span style="color:#666">.</span>keys())
        <span style="color:#a2f;font-weight:bold">for</span> node <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>nodes:
            node<span style="color:#666">.</span>prune_deps()

        self<span style="color:#666">.</span>name_to_node <span style="color:#666">=</span> {n<span style="color:#666">.</span>get_name(): n <span style="color:#a2f;font-weight:bold">for</span> n <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>nodes}
        self<span style="color:#666">.</span>name_to_fused_node <span style="color:#666">=</span> None  <span style="color:#080;font-style:italic"># set in fuse_nods()</span>

        <span style="color:#080;font-style:italic"># we handle mutation by renaming modified versions of the same</span>
        <span style="color:#080;font-style:italic"># buffer in the dependency graph to prevent cycles.</span>
        <span style="color:#080;font-style:italic"># mutation_renames: tracks the current name for a given buffer</span>
        <span style="color:#080;font-style:italic">#                   (changed once per mutation)</span>
        self<span style="color:#666">.</span>mutation_real_name <span style="color:#666">=</span> {}
        <span style="color:#080;font-style:italic"># mutation_real_name: maps back to the original name for codegen</span>
        self<span style="color:#666">.</span>mutation_renames <span style="color:#666">=</span> {}

        <span style="color:#080;font-style:italic"># NOTE. 添加node间依赖边；先进行alias分析，消除alias；处理mutation</span>
        self<span style="color:#666">.</span>compute_dependencies()
        <span style="color:#080;font-style:italic"># NOTE. 后序遍历nodes，得到拓扑序的排序。</span>
        self<span style="color:#666">.</span>topological_sort_schedule()
        <span style="color:#080;font-style:italic"># NOTE. 消除没有user的节点</span>
        self<span style="color:#666">.</span>dead_node_elimination()
        <span style="color:#080;font-style:italic"># NOTE. 前面已经进行了依赖分析和拓扑排序，递归的统计每个节点的前驱节点，用于后面的fusion等分析。</span>
        self<span style="color:#666">.</span>compute_predecessors()

        metrics<span style="color:#666">.</span>ir_nodes_pre_fusion <span style="color:#666">+=</span> <span style="color:#a2f">len</span>(self<span style="color:#666">.</span>nodes)
        V<span style="color:#666">.</span>debug<span style="color:#666">.</span>ir_pre_fusion(self<span style="color:#666">.</span>nodes)
        self<span style="color:#666">.</span>num_orig_nodes <span style="color:#666">=</span> <span style="color:#a2f">len</span>(self<span style="color:#666">.</span>nodes)
        self<span style="color:#666">.</span>name_to_fused_node <span style="color:#666">=</span> {n<span style="color:#666">.</span>get_name(): n <span style="color:#a2f;font-weight:bold">for</span> n <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>nodes}
        <span style="color:#080;font-style:italic"># NOTE. 生成ForeachNodeScheduler, 用于lowering 例如torch._foreach_add op，便于对这类算子的horizontal fusion.</span>
        <span style="color:#080;font-style:italic"># NOTE. https://docs.google.com/document/d/1JLr5yMAR8TuKW78ixKeqzfDHhcazwxKo_JXQnP_-wyY/edit?kh_source=GDOCS#heading=h.8x4z4mmet3im</span>
        self<span style="color:#666">.</span>create_foreach_nodes()
        self<span style="color:#666">.</span>topological_sort_schedule()
        <span style="color:#080;font-style:italic"># NOTE. 对 ir.Buffer（SchedulerNode） 进行fuse.</span>
        self<span style="color:#666">.</span>fuse_nodes()
        <span style="color:#080;font-style:italic"># NOTE. 递归的更新 node的last_usage，</span>
        self<span style="color:#666">.</span>compute_last_usage()
        V<span style="color:#666">.</span>debug<span style="color:#666">.</span>ir_post_fusion(self<span style="color:#666">.</span>nodes)
        V<span style="color:#666">.</span>debug<span style="color:#666">.</span>graph_diagram(self<span style="color:#666">.</span>nodes)
        self<span style="color:#666">.</span>debug_draw_graph()

        <span style="color:#080;font-style:italic"># used during codegen:</span>
        self<span style="color:#666">.</span>current_device <span style="color:#666">=</span> None
        self<span style="color:#666">.</span>buffer_names_to_free <span style="color:#666">=</span> <span style="color:#a2f">set</span>()
        self<span style="color:#666">.</span>buffer_names_no_longer_needed <span style="color:#666">=</span> <span style="color:#a2f">set</span>()

        <span style="color:#080;font-style:italic"># fx graph node to the position it appears in the graph</span>
        <span style="color:#080;font-style:italic"># for debug attribution</span>
        self<span style="color:#666">.</span>origin_to_index <span style="color:#666">=</span> {}

        log<span style="color:#666">.</span>info(<span style="color:#b44">&#34;Number of scheduler nodes after fusion </span><span style="color:#b68;font-weight:bold">%d</span><span style="color:#b44">&#34;</span>, <span style="color:#a2f">len</span>(self<span style="color:#666">.</span>nodes))
</code></pre></div><ul>
<li>
<h2 id="fuse_nodes">fuse_nodes</h2>
</li>
</ul>
<p>对用到相同buffer的node尝试进行fusion。
TODO more detail</p>
<ul>
<li>
<h2 id="codegen">codegen</h2>
</li>
</ul>
<p>将nodes compile到triton kernel（如果是cuda 后端）。<code>ir.Buffer</code>在<code>__init__</code>中被转换为不同类型的SchedulerNode, <code>codegen</code>函数则分别将这些nodes逐个编译成triton kernel。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f">@dynamo_timed</span>
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">codegen</span>(self):
        <span style="color:#a2f;font-weight:bold">for</span> node <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>nodes:
            <span style="color:#080;font-style:italic"># NOTE. wrapper_code 相关，TODO</span>
            self<span style="color:#666">.</span>enter_context(node)
            self<span style="color:#666">.</span>buffer_names_no_longer_needed<span style="color:#666">.</span>update(node<span style="color:#666">.</span>last_usage)

            <span style="color:#080;font-style:italic"># NOTE. codegen前获取这个node对应的device</span>
            <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f;font-weight:bold">not</span> <span style="color:#a2f">isinstance</span>(node, NopKernelSchedulerNode):
                device <span style="color:#666">=</span> node<span style="color:#666">.</span>get_device()
                <span style="color:#a2f;font-weight:bold">if</span> (
                    device <span style="color:#666">!=</span> self<span style="color:#666">.</span>current_device
                    <span style="color:#a2f;font-weight:bold">or</span> node<span style="color:#666">.</span>is_extern()
                    <span style="color:#a2f;font-weight:bold">or</span> node<span style="color:#666">.</span>is_template()
                ):
                    self<span style="color:#666">.</span>flush()
                <span style="color:#a2f;font-weight:bold">if</span> device <span style="color:#666">!=</span> self<span style="color:#666">.</span>current_device:
                    <span style="color:#a2f;font-weight:bold">if</span> device<span style="color:#666">.</span>type <span style="color:#666">==</span> <span style="color:#b44">&#34;cuda&#34;</span>:
                        <span style="color:#a2f;font-weight:bold">if</span> self<span style="color:#666">.</span>current_device <span style="color:#a2f;font-weight:bold">and</span> self<span style="color:#666">.</span>current_device<span style="color:#666">.</span>type <span style="color:#666">==</span> <span style="color:#b44">&#34;cuda&#34;</span>:
                            V<span style="color:#666">.</span>graph<span style="color:#666">.</span>wrapper_code<span style="color:#666">.</span>codegen_device_guard_exit()
                        <span style="color:#a2f;font-weight:bold">assert</span> device<span style="color:#666">.</span>index <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> None, <span style="color:#b44">&#34;device should have an index&#34;</span>
                        V<span style="color:#666">.</span>graph<span style="color:#666">.</span>wrapper_code<span style="color:#666">.</span>codegen_device_guard_enter(device<span style="color:#666">.</span>index)
                    <span style="color:#a2f;font-weight:bold">elif</span> self<span style="color:#666">.</span>current_device <span style="color:#a2f;font-weight:bold">and</span> self<span style="color:#666">.</span>current_device<span style="color:#666">.</span>type <span style="color:#666">==</span> <span style="color:#b44">&#34;cuda&#34;</span>:
                        V<span style="color:#666">.</span>graph<span style="color:#666">.</span>wrapper_code<span style="color:#666">.</span>codegen_device_guard_exit()
                    self<span style="color:#666">.</span>current_device <span style="color:#666">=</span> device

            self<span style="color:#666">.</span>buffer_names_to_free<span style="color:#666">.</span>update(node<span style="color:#666">.</span>last_usage)

            <span style="color:#080;font-style:italic"># NOTE. 根据 node 类型选择对应的codegen实现。</span>
            <span style="color:#080;font-style:italic"># NOTE. template -&gt; codegen_template</span>
            <span style="color:#080;font-style:italic"># NOTE. extern   -&gt; codegen_extern_call</span>
            <span style="color:#080;font-style:italic"># NOTE. foreach  -&gt; codegen_foreach</span>
            <span style="color:#080;font-style:italic"># NOTE. FusedSchedulerNode, SchedulerNode -&gt; codegen_nodes</span>
            <span style="color:#a2f;font-weight:bold">if</span> node<span style="color:#666">.</span>is_template():
                node, <span style="color:#666">*</span>epilogue <span style="color:#666">=</span> node<span style="color:#666">.</span>get_nodes()
                self<span style="color:#666">.</span>get_backend(device)<span style="color:#666">.</span>codegen_template(node, epilogue)
            <span style="color:#a2f;font-weight:bold">elif</span> node<span style="color:#666">.</span>is_extern():
                self<span style="color:#666">.</span>codegen_extern_call(node)
            <span style="color:#a2f;font-weight:bold">elif</span> node<span style="color:#666">.</span>is_foreach():
                self<span style="color:#666">.</span>get_backend(device)<span style="color:#666">.</span>codegen_foreach(node)
            <span style="color:#a2f;font-weight:bold">elif</span> <span style="color:#a2f">isinstance</span>(node, (FusedSchedulerNode, SchedulerNode)):
                self<span style="color:#666">.</span>get_backend(device)<span style="color:#666">.</span>codegen_nodes(node<span style="color:#666">.</span>get_nodes())
            <span style="color:#a2f;font-weight:bold">else</span>:
                <span style="color:#a2f;font-weight:bold">assert</span> <span style="color:#a2f">isinstance</span>(node, NopKernelSchedulerNode)
                node<span style="color:#666">.</span>allocate()

            <span style="color:#a2f;font-weight:bold">if</span> config<span style="color:#666">.</span>triton<span style="color:#666">.</span>debug_sync_kernel:
                self<span style="color:#666">.</span>get_backend(device)<span style="color:#666">.</span>codegen_sync()

            self<span style="color:#666">.</span>available_buffer_names<span style="color:#666">.</span>update(node<span style="color:#666">.</span>get_names())

        self<span style="color:#666">.</span>flush()
</code></pre></div><ul>
<li>
<h2 id="tritonscheduling">TritonScheduling</h2>
</li>
</ul>
<p>当device='cuda'时，<code>get_backend</code>会返回<code>TritonScheduling</code>对象，用于将node编译为triton kernel。</p>
<ul>
<li>
<h2 id="codegen_template">codegen_template</h2>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">codegen_template</span>(self, template_node, epilogue_nodes):
        <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">        Codegen a triton template
</span><span style="color:#b44">        &#34;&#34;&#34;</span>
        _, (numel, rnumel) <span style="color:#666">=</span> template_node<span style="color:#666">.</span>group
        <span style="color:#a2f;font-weight:bold">assert</span> rnumel <span style="color:#666">==</span> <span style="color:#666">1</span>
        kernel, render <span style="color:#666">=</span> template_node<span style="color:#666">.</span>node<span style="color:#666">.</span>make_kernel_render(template_node<span style="color:#666">.</span>node)
        <span style="color:#a2f;font-weight:bold">with</span> kernel:
            <span style="color:#a2f;font-weight:bold">for</span> node <span style="color:#a2f;font-weight:bold">in</span> [template_node, <span style="color:#666">*</span>epilogue_nodes]:
                node<span style="color:#666">.</span>mark_run()
            partial_code <span style="color:#666">=</span> render()
            <span style="color:#a2f;font-weight:bold">for</span> node <span style="color:#a2f;font-weight:bold">in</span> epilogue_nodes:
                node<span style="color:#666">.</span>codegen(kernel<span style="color:#666">.</span>split_and_set_ranges(node<span style="color:#666">.</span>get_ranges()))

        <span style="color:#080;font-style:italic"># finalize must be called after adding epilogue above</span>
        src_code <span style="color:#666">=</span> partial_code<span style="color:#666">.</span>finalize()
        node_schedule <span style="color:#666">=</span> [template_node, <span style="color:#666">*</span>epilogue_nodes]
        kernel_name <span style="color:#666">=</span> self<span style="color:#666">.</span>define_kernel(src_code, node_schedule)
        self<span style="color:#666">.</span>codegen_comment(node_schedule)
        kernel<span style="color:#666">.</span>call_kernel(kernel_name)
        self<span style="color:#666">.</span>scheduler<span style="color:#666">.</span>free_buffers()
</code></pre></div><ul>
<li>
<h2 id="codegen_extern_call">codegen_extern_call</h2>
</li>
</ul>
<p>最终会调用<code>ir</code>中的codegen方法。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">codegen_extern_call</span>(self, scheduler_node: ExternKernelSchedulerNode):
        <span style="color:#a2f;font-weight:bold">assert</span> <span style="color:#a2f">isinstance</span>(scheduler_node, ExternKernelSchedulerNode)
        scheduler_node<span style="color:#666">.</span>allocate()
        node <span style="color:#666">=</span> scheduler_node<span style="color:#666">.</span>node
        node<span style="color:#666">.</span>codegen(V<span style="color:#666">.</span>graph<span style="color:#666">.</span>wrapper_code)
        self<span style="color:#666">.</span>free_buffers()
</code></pre></div><ul>
<li>
<h2 id="codegen_foreach">codegen_foreach</h2>
</li>
</ul>
<p><a href="https://docs.google.com/document/d/1JLr5yMAR8TuKW78ixKeqzfDHhcazwxKo_JXQnP_-wyY/edit?kh_source=GDOCS#heading=h.9mcian2hhqpg">https://docs.google.com/document/d/1JLr5yMAR8TuKW78ixKeqzfDHhcazwxKo_JXQnP_-wyY/edit?kh_source=GDOCS#heading=h.9mcian2hhqpg</a></p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">codegen_foreach</span>(self, foreach_node):
        <span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">.triton_foreach</span> <span style="color:#a2f;font-weight:bold">import</span> ForeachKernel

        <span style="color:#a2f;font-weight:bold">for</span> partitions_with_metadata <span style="color:#a2f;font-weight:bold">in</span> ForeachKernel<span style="color:#666">.</span>horizontal_partition(
            foreach_node<span style="color:#666">.</span>get_subkernel_nodes(), self
        ):
            kernel <span style="color:#666">=</span> ForeachKernel()
            <span style="color:#a2f;font-weight:bold">for</span> nodes, tiled_groups, numel, rnumel <span style="color:#a2f;font-weight:bold">in</span> partitions_with_metadata:
                node_schedule <span style="color:#666">=</span> self<span style="color:#666">.</span>generate_node_schedule(nodes, numel, rnumel)
                (
                    reduction_hint_val,
                    mutations,
                    index_dtype,
                ) <span style="color:#666">=</span> self<span style="color:#666">.</span>get_kernel_args(node_schedule, numel, rnumel)
                self<span style="color:#666">.</span>codegen_node_schedule_with_kernel(
                    node_schedule,
                    kernel<span style="color:#666">.</span>create_sub_kernel(
                        <span style="color:#666">*</span>tiled_groups,
                        reduction_hint<span style="color:#666">=</span>reduction_hint_val,
                        mutations<span style="color:#666">=</span>mutations,
                        index_dtype<span style="color:#666">=</span>index_dtype,
                    ),
                )

            src_code <span style="color:#666">=</span> kernel<span style="color:#666">.</span>codegen_kernel()
            kernel_name <span style="color:#666">=</span> self<span style="color:#666">.</span>define_kernel(src_code, [foreach_node])
            self<span style="color:#666">.</span>codegen_comment([foreach_node])
            kernel<span style="color:#666">.</span>call_kernel(V<span style="color:#666">.</span>graph<span style="color:#666">.</span>wrapper_code, kernel_name)

        self<span style="color:#666">.</span>scheduler<span style="color:#666">.</span>free_buffers()
</code></pre></div><ul>
<li>
<h2 id="codegen_nodes">codegen_nodes</h2>
</li>
</ul>
<p>调用<code>codegen_node_schedule</code>生成triton kernel，</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">codegen_nodes</span>(self, nodes):
        <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">        Given a set of pre-fused nodes, generate a Triton kernel.
</span><span style="color:#b44">        &#34;&#34;&#34;</span>
        _, (numel, rnumel) <span style="color:#666">=</span> <span style="color:#a2f">max</span>(nodes, key<span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">lambda</span> x: <span style="color:#a2f">int</span>(x<span style="color:#666">.</span>is_reduction()))<span style="color:#666">.</span>group

        node_schedule <span style="color:#666">=</span> self<span style="color:#666">.</span>generate_node_schedule(nodes, numel, rnumel)

        <span style="color:#a2f;font-weight:bold">if</span> schedule_log<span style="color:#666">.</span>isEnabledFor(logging<span style="color:#666">.</span>DEBUG):
            schedule_log<span style="color:#666">.</span>debug(<span style="color:#b44">&#34;Schedule:</span><span style="color:#b62;font-weight:bold">\n</span><span style="color:#b44"> </span><span style="color:#b68;font-weight:bold">%s</span><span style="color:#b44">&#34;</span>, node_schedule)

        <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>codegen_node_schedule(node_schedule, numel, rnumel)
</code></pre></div><p>triton kernel具体的生成是在做字符串拼接：</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">codegen_kernel</span>(self, name<span style="color:#666">=</span>None):
        <span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">triton</span> <span style="color:#a2f;font-weight:bold">import</span> next_power_of_2

        code <span style="color:#666">=</span> IndentedBuffer()

        size_hints <span style="color:#666">=</span> [
            next_power_of_2(V<span style="color:#666">.</span>graph<span style="color:#666">.</span>sizevars<span style="color:#666">.</span>size_hint(numel)) <span style="color:#a2f;font-weight:bold">for</span> numel <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>numels
        ]
        <span style="color:#a2f;font-weight:bold">if</span> self<span style="color:#666">.</span>persistent_reduction:
            <span style="color:#a2f;font-weight:bold">assert</span> self<span style="color:#666">.</span>inside_reduction
            heuristics <span style="color:#666">=</span> <span style="color:#b44">&#34;persistent_reduction&#34;</span>
        <span style="color:#a2f;font-weight:bold">elif</span> self<span style="color:#666">.</span>inside_reduction:
            heuristics <span style="color:#666">=</span> <span style="color:#b44">&#34;reduction&#34;</span>
        <span style="color:#a2f;font-weight:bold">else</span>:
            size_hints<span style="color:#666">.</span>pop()
            heuristics <span style="color:#666">=</span> <span style="color:#b44">&#34;pointwise&#34;</span>

        <span style="color:#a2f;font-weight:bold">if</span> name <span style="color:#a2f;font-weight:bold">is</span> None:
            code<span style="color:#666">.</span>splice(
                f<span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">                    import triton
</span><span style="color:#b44">                    import triton.language as tl
</span><span style="color:#b44">                    from torch._inductor.ir import ReductionHint
</span><span style="color:#b44">                    from torch._inductor.ir import TileHint
</span><span style="color:#b44">                    from torch._inductor.triton_heuristics import AutotuneHint, {heuristics}
</span><span style="color:#b44">                    from torch._inductor.utils import instance_descriptor
</span><span style="color:#b44">                    from torch._inductor import triton_helpers
</span><span style="color:#b44">                &#34;&#34;&#34;</span>
            )
            <span style="color:#a2f;font-weight:bold">if</span> config<span style="color:#666">.</span>benchmark_kernel:
                code<span style="color:#666">.</span>splice(
                    <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">                        from torch._dynamo.testing import rand_strided
</span><span style="color:#b44">                        from torch._C import _cuda_getCurrentRawStream as get_cuda_stream
</span><span style="color:#b44">                        import torch
</span><span style="color:#b44">                        from torch._inductor.triton_heuristics import grid
</span><span style="color:#b44">                    &#34;&#34;&#34;</span>
                )

        argdefs, _, signature <span style="color:#666">=</span> self<span style="color:#666">.</span>args<span style="color:#666">.</span>python_argdefs()
        <span style="color:#080;font-style:italic"># maps actual expression to SizeArg if its in sizevars replacements</span>
        <span style="color:#a2f;font-weight:bold">for</span> i, arg <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">enumerate</span>(signature):
            <span style="color:#a2f;font-weight:bold">if</span> (
                <span style="color:#a2f">isinstance</span>(arg, SizeArg)
                <span style="color:#a2f;font-weight:bold">and</span> arg<span style="color:#666">.</span>expr <span style="color:#a2f;font-weight:bold">in</span> V<span style="color:#666">.</span>graph<span style="color:#666">.</span>sizevars<span style="color:#666">.</span>inv_precomputed_replacements
            ):
                signature[i] <span style="color:#666">=</span> SizeArg(
                    arg<span style="color:#666">.</span>name, V<span style="color:#666">.</span>graph<span style="color:#666">.</span>sizevars<span style="color:#666">.</span>inv_precomputed_replacements[arg<span style="color:#666">.</span>expr]
                )

        mutated_args <span style="color:#666">=</span> <span style="color:#a2f">set</span>()
        <span style="color:#a2f;font-weight:bold">for</span> mutation <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>mutations:
            <span style="color:#a2f;font-weight:bold">if</span> mutation <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>args<span style="color:#666">.</span>input_buffers:
                mutated_args<span style="color:#666">.</span>add(self<span style="color:#666">.</span>args<span style="color:#666">.</span>input_buffers[mutation])
            <span style="color:#a2f;font-weight:bold">if</span> (
                mutation <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>args<span style="color:#666">.</span>inplace_buffers
                <span style="color:#a2f;font-weight:bold">and</span> mutation <span style="color:#a2f;font-weight:bold">not</span> <span style="color:#a2f;font-weight:bold">in</span> V<span style="color:#666">.</span>graph<span style="color:#666">.</span>removed_buffers
            ):
                mutated_args<span style="color:#666">.</span>add(self<span style="color:#666">.</span>args<span style="color:#666">.</span>inplace_buffers[mutation]<span style="color:#666">.</span>inner_name)
            <span style="color:#a2f;font-weight:bold">if</span> mutation <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>args<span style="color:#666">.</span>output_buffers:
                mutated_args<span style="color:#666">.</span>add(self<span style="color:#666">.</span>args<span style="color:#666">.</span>output_buffers[mutation])
        mutated_args <span style="color:#666">=</span> <span style="color:#a2f">sorted</span>(mutated_args)

        triton_meta <span style="color:#666">=</span> {
            <span style="color:#b44">&#34;signature&#34;</span>: signature_to_meta(signature, size_dtype<span style="color:#666">=</span>self<span style="color:#666">.</span>index_dtype),
            <span style="color:#b44">&#34;device&#34;</span>: V<span style="color:#666">.</span>graph<span style="color:#666">.</span>scheduler<span style="color:#666">.</span>current_device<span style="color:#666">.</span>index,
            <span style="color:#b44">&#34;device_type&#34;</span>: V<span style="color:#666">.</span>graph<span style="color:#666">.</span>scheduler<span style="color:#666">.</span>current_device<span style="color:#666">.</span>type,
            <span style="color:#b44">&#34;constants&#34;</span>: {},
            <span style="color:#b44">&#34;mutated_arg_names&#34;</span>: mutated_args,
            <span style="color:#b44">&#34;autotune_hints&#34;</span>: <span style="color:#a2f">set</span>(self<span style="color:#666">.</span>autotune_hints),
            <span style="color:#b44">&#34;kernel_name&#34;</span>: <span style="color:#b44">&#34;DESCRIPTIVE_KRNL_NAME&#34;</span>,
        }

        <span style="color:#a2f;font-weight:bold">for</span> tree <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>range_trees:
            <span style="color:#a2f;font-weight:bold">if</span> tree<span style="color:#666">.</span>prefix <span style="color:#666">!=</span> <span style="color:#b44">&#34;r&#34;</span> <span style="color:#a2f;font-weight:bold">or</span> self<span style="color:#666">.</span>inside_reduction:
                sizearg <span style="color:#666">=</span> SizeArg(f<span style="color:#b44">&#34;{tree.prefix}numel&#34;</span>, tree<span style="color:#666">.</span>numel)
                signature<span style="color:#666">.</span>append(sizearg)
                triton_meta[<span style="color:#b44">&#34;signature&#34;</span>][<span style="color:#a2f">len</span>(argdefs)] <span style="color:#666">=</span> signature_of(
                    sizearg, size_dtype<span style="color:#666">=</span>self<span style="color:#666">.</span>index_dtype
                )
                argdefs<span style="color:#666">.</span>append(f<span style="color:#b44">&#34;{tree.prefix}numel&#34;</span>)
                <span style="color:#080;font-style:italic"># constexpr version causes issues, see</span>
                <span style="color:#080;font-style:italic"># https://github.com/pytorch/torchdynamo/pull/1362</span>
                <span style="color:#080;font-style:italic"># triton_meta[&#34;constants&#34;][len(argdefs)] = V.graph.sizevars.size_hint(</span>
                <span style="color:#080;font-style:italic">#     tree.numel</span>
                <span style="color:#080;font-style:italic"># )</span>
                <span style="color:#080;font-style:italic"># argdefs.append(f&#34;{tree.prefix}numel: tl.constexpr&#34;)</span>
        triton_meta[<span style="color:#b44">&#34;configs&#34;</span>] <span style="color:#666">=</span> [config_of(signature)]

        <span style="color:#a2f;font-weight:bold">for</span> tree <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>range_trees:
            <span style="color:#a2f;font-weight:bold">if</span> tree<span style="color:#666">.</span>prefix <span style="color:#666">==</span> <span style="color:#b44">&#34;r&#34;</span> <span style="color:#a2f;font-weight:bold">and</span> (
                <span style="color:#a2f;font-weight:bold">not</span> self<span style="color:#666">.</span>inside_reduction <span style="color:#a2f;font-weight:bold">or</span> self<span style="color:#666">.</span>persistent_reduction
            ):
                <span style="color:#a2f;font-weight:bold">continue</span>
            <span style="color:#a2f;font-weight:bold">if</span> tree<span style="color:#666">.</span>prefix <span style="color:#666">==</span> <span style="color:#b44">&#34;x&#34;</span> <span style="color:#a2f;font-weight:bold">and</span> self<span style="color:#666">.</span>no_x_dim:
                <span style="color:#a2f;font-weight:bold">continue</span>
            argdefs<span style="color:#666">.</span>append(f<span style="color:#b44">&#34;{tree.prefix.upper()}BLOCK : tl.constexpr&#34;</span>)

        <span style="color:#a2f;font-weight:bold">if</span> self<span style="color:#666">.</span>inside_reduction:
            reduction_hint <span style="color:#666">=</span> self<span style="color:#666">.</span>reduction_hint
            heuristics_line <span style="color:#666">=</span> f<span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">                @{heuristics}(
</span><span style="color:#b44">                    size_hints={size_hints!r},
</span><span style="color:#b44">                    reduction_hint={reduction_hint},
</span><span style="color:#b44">                    filename=__file__,
</span><span style="color:#b44">                    meta={triton_meta!r}
</span><span style="color:#b44">                )
</span><span style="color:#b44">                @triton.jit
</span><span style="color:#b44">            &#34;&#34;&#34;</span>
        <span style="color:#a2f;font-weight:bold">else</span>:
            tile_hint <span style="color:#666">=</span> <span style="color:#b44">&#34;&#34;</span>
            <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">len</span>(size_hints) <span style="color:#666">==</span> <span style="color:#666">2</span>:
                <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">len</span>(signature) <span style="color:#666">==</span> <span style="color:#666">4</span>:  <span style="color:#080;font-style:italic"># input, output and 2 args</span>
                    tile_hint <span style="color:#666">=</span> <span style="color:#b44">&#34;tile_hint=TileHint.SQUARE,&#34;</span>
                <span style="color:#a2f;font-weight:bold">else</span>:
                    tile_hint <span style="color:#666">=</span> <span style="color:#b44">&#34;tile_hint=TileHint.DEFAULT,&#34;</span>
            heuristics_line <span style="color:#666">=</span> f<span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">                @{heuristics}(size_hints={size_hints!r}, {tile_hint}filename=__file__, meta={triton_meta!r})
</span><span style="color:#b44">                @triton.jit
</span><span style="color:#b44">            &#34;&#34;&#34;</span>
        code<span style="color:#666">.</span>splice(heuristics_line)
        code<span style="color:#666">.</span>writeline(f<span style="color:#b44">&#34;def {name or &#39;KERNEL_NAME&#39;}({&#39;, &#39;.join(argdefs)}):&#34;</span>)
        self<span style="color:#666">.</span>codegen_body()
        <span style="color:#a2f;font-weight:bold">with</span> code<span style="color:#666">.</span>indent():
            self<span style="color:#666">.</span>codegen_static_numels(code)
            <span style="color:#a2f;font-weight:bold">for</span> old, new <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>args<span style="color:#666">.</span>aliases():
                code<span style="color:#666">.</span>writeline(f<span style="color:#b44">&#34;{old} = {new}&#34;</span>)
            code<span style="color:#666">.</span>splice(self<span style="color:#666">.</span>body)

        <span style="color:#a2f;font-weight:bold">if</span> config<span style="color:#666">.</span>benchmark_kernel:
            code<span style="color:#666">.</span>splice(self<span style="color:#666">.</span>codegen_kernel_benchmark())

        <span style="color:#a2f;font-weight:bold">if</span> name <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> None:
            <span style="color:#a2f;font-weight:bold">return</span> code<span style="color:#666">.</span>getvalue()

        <span style="color:#a2f;font-weight:bold">return</span> code<span style="color:#666">.</span>getvalue()
</code></pre></div><ul>
<li>
<h2 id="node">Node</h2>
</li>
<li>
<h3 id="baseschedulernode">BaseSchedulerNode</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">BaseSchedulerNode</span>:
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self, scheduler: <span style="color:#b44">&#34;Scheduler&#34;</span>, node: ir<span style="color:#666">.</span>Buffer):
        self<span style="color:#666">.</span>scheduler: Scheduler <span style="color:#666">=</span> scheduler
        self<span style="color:#666">.</span>node: ir<span style="color:#666">.</span>Buffer <span style="color:#666">=</span> node
        self<span style="color:#666">.</span>users: Optional[List[NodeUser]] <span style="color:#666">=</span> None
        self<span style="color:#666">.</span>inverse_users: List[BaseSchedulerNode] <span style="color:#666">=</span> []
        self<span style="color:#666">.</span>set_read_writes(node<span style="color:#666">.</span>get_read_writes())
        self<span style="color:#666">.</span>recursive_predecessors: Optional[Set[<span style="color:#a2f">str</span>]] <span style="color:#666">=</span> None
        self<span style="color:#666">.</span>min_order: Optional[<span style="color:#a2f">int</span>] <span style="color:#666">=</span> None
        self<span style="color:#666">.</span>max_order: Optional[<span style="color:#a2f">int</span>] <span style="color:#666">=</span> None
        self<span style="color:#666">.</span>last_usage: Set[<span style="color:#a2f">str</span>] <span style="color:#666">=</span> None  <span style="color:#080;font-style:italic"># buffers that won&#39;t be used after this kernel</span>
        self<span style="color:#666">.</span>written <span style="color:#666">=</span> False
</code></pre></div><ul>
<li>
<h3 id="externkernelschedulernode">ExternKernelSchedulerNode</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">ExternKernelSchedulerNode</span>(BaseSchedulerNode):
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">debug_str_extra</span>(self) <span style="color:#666">-&gt;</span> <span style="color:#a2f">str</span>:
        <span style="color:#a2f;font-weight:bold">return</span> f<span style="color:#b44">&#34;{self.get_name()}.node.kernel = {getattr(self.node, &#39;kernel&#39;, None)}&#34;</span>

    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">is_extern</span>(self):
        <span style="color:#a2f;font-weight:bold">return</span> True

    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">has_side_effects</span>(self):
        <span style="color:#a2f;font-weight:bold">return</span> <span style="color:#a2f">hasattr</span>(self<span style="color:#666">.</span>node, <span style="color:#b44">&#34;has_side_effects&#34;</span>) <span style="color:#a2f;font-weight:bold">and</span> self<span style="color:#666">.</span>node<span style="color:#666">.</span>has_side_effects()

    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">can_inplace</span>(self, read_dep: dependencies<span style="color:#666">.</span>MemoryDep):
        <span style="color:#a2f;font-weight:bold">if</span> self<span style="color:#666">.</span>get_aliases() <span style="color:#a2f;font-weight:bold">or</span> self<span style="color:#666">.</span>is_template():
            <span style="color:#a2f;font-weight:bold">return</span> False

        <span style="color:#a2f;font-weight:bold">if</span> read_dep<span style="color:#666">.</span>name <span style="color:#a2f;font-weight:bold">not</span> <span style="color:#a2f;font-weight:bold">in</span> self<span style="color:#666">.</span>scheduler<span style="color:#666">.</span>name_to_node:
            <span style="color:#080;font-style:italic"># don&#39;t allow reuse of an &#39;input&#39; buffer, we don&#39;t own it</span>
            <span style="color:#080;font-style:italic"># (would this have been fixed if I tracked mutations properly above?)</span>
            <span style="color:#a2f;font-weight:bold">return</span> False

        <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f;font-weight:bold">not</span> <span style="color:#a2f">isinstance</span>(
            self<span style="color:#666">.</span>node, (torch<span style="color:#666">.</span>_inductor<span style="color:#666">.</span>ir<span style="color:#666">.</span>AllReduce, torch<span style="color:#666">.</span>_inductor<span style="color:#666">.</span>ir<span style="color:#666">.</span>InPlaceHint)
        ):
            <span style="color:#080;font-style:italic"># TODO make this a property of the IR</span>
            <span style="color:#a2f;font-weight:bold">return</span> False

        <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">len</span>(self<span style="color:#666">.</span>read_writes<span style="color:#666">.</span>writes) <span style="color:#666">==</span> <span style="color:#666">1</span>:
            write_dep <span style="color:#666">=</span> <span style="color:#a2f">next</span>(<span style="color:#a2f">iter</span>(self<span style="color:#666">.</span>read_writes<span style="color:#666">.</span>writes))
            <span style="color:#a2f;font-weight:bold">return</span> read_dep<span style="color:#666">.</span>numbytes_hint() <span style="color:#666">==</span> write_dep<span style="color:#666">.</span>numbytes_hint()

        <span style="color:#a2f;font-weight:bold">return</span> False
</code></pre></div><ul>
<li>
<h3 id="schedulernode">SchedulerNode</h3>
</li>
</ul>
<p>只有ir.ComputedBuffer才转换到SchedulerNode</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">SchedulerNode</span>(BaseSchedulerNode):
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self, scheduler: <span style="color:#b44">&#34;Scheduler&#34;</span>, node: ir<span style="color:#666">.</span>ComputedBuffer, group_fn):
        <span style="color:#a2f">super</span>()<span style="color:#666">.</span>__init__(scheduler, node)
        (
            self<span style="color:#666">.</span>_sizes,
            self<span style="color:#666">.</span>_body,
        ) <span style="color:#666">=</span> node<span style="color:#666">.</span>simplify_and_reorder()

        self<span style="color:#666">.</span>group <span style="color:#666">=</span> (node<span style="color:#666">.</span>get_device(), group_fn(self<span style="color:#666">.</span>_sizes))

        <span style="color:#a2f;font-weight:bold">if</span> self<span style="color:#666">.</span>is_template():
            self<span style="color:#666">.</span>set_read_writes(node<span style="color:#666">.</span>normalized_read_writes())
        <span style="color:#a2f;font-weight:bold">else</span>:
            self<span style="color:#666">.</span>set_read_writes(
                dependencies<span style="color:#666">.</span>extract_read_writes(
                    self<span style="color:#666">.</span>_body, <span style="color:#666">*</span>self<span style="color:#666">.</span>_sizes, normalize<span style="color:#666">=</span>True
                )
            )
</code></pre></div><ul>
<li>
<h3 id="nopkernelschedulernode">NopKernelSchedulerNode</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">NopKernelSchedulerNode</span>(BaseSchedulerNode):
    <span style="color:#a2f;font-weight:bold">pass</span>
</code></pre></div><ul>
<li>
<h3 id="fusedschedulernode">FusedSchedulerNode</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">FusedSchedulerNode</span>(BaseSchedulerNode):
    <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">    This is a &#34;fake&#34; scheduler node that represents a group of scheduler nodes
</span><span style="color:#b44">    that are meant to be fused together. The way it does this is by maintaining
</span><span style="color:#b44">    its unmet dependencies as the union of its constituent nodes.
</span><span style="color:#b44">    &#34;&#34;&#34;</span>
</code></pre></div><h1 id="aot_autograd">aot_autograd</h1>
<p>TODO 反向图构建和图切分</p>
<h1 id="lowering">lowering</h1>
<ul>
<li>
<h2 id="lowerings">lowerings</h2>
</li>
</ul>
<p><code>lowering.py</code> 中实现了aten ops到Inductor IR(loop-level-ir)的映射，每个ops会被转换成 <code>TensorBox</code>或者<code>View</code>，前者代表这个op会产生新的tensor，而后者表示op的返回值是已有的tensor的view。op的计算逻辑保存在<code>inner_fn</code>中，op的类型有两个大类，分别是Pointwise和Reduction，两种在循环上行为不同的类型（很难fuse在一起）。</p>
<ul>
<li>
<h3 id="register_lowering">register_lowering</h3>
</li>
</ul>
<p>用于注册lowering的实现，将其存到全局 map <code>lowerings</code>中。</p>
<ul>
<li>
<h3 id="make_pointwise">make_pointwise</h3>
</li>
</ul>
<p>创建pointwise类型op的表达，即实例化一个<code>Pointwise</code>对象。其核心是构造<code>inner_fn</code>，因为pointwise类型算子的行为几乎是一样的，所以这个函数就像模板函数一样为pointwise ops实例化Inductor IR。</p>
<ul>
<li>
<h3 id="make_reduction">make_reduction</h3>
</li>
</ul>
<p>同样的，<code>make_reduction</code>函数用于实例化<code>Reduction</code>对象，相比<code>Pointwise</code>多了reduce轴的信息。</p>
<ul>
<li>
<h3 id="xxxview">xxxView</h3>
</li>
</ul>
<p>对于viewlike-ops，则实例化<code>View</code>对象，如：</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f">@register_lowering</span>(aten<span style="color:#666">.</span>_unsafe_view, type_promotion_kind<span style="color:#666">=</span>None)
<span style="color:#a2f">@register_lowering</span>(aten<span style="color:#666">.</span>view, type_promotion_kind<span style="color:#666">=</span>None)
<span style="color:#a2f">@register_lowering</span>(aten<span style="color:#666">.</span>reshape, type_promotion_kind<span style="color:#666">=</span>None)
<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">view</span>(x, sizes):
    <span style="color:#a2f;font-weight:bold">assert</span> <span style="color:#a2f">isinstance</span>(x, TensorBox)
    <span style="color:#a2f;font-weight:bold">assert</span> <span style="color:#a2f">isinstance</span>(sizes, (<span style="color:#a2f">list</span>, <span style="color:#a2f">tuple</span>))
    <span style="color:#a2f;font-weight:bold">return</span> TensorBox(View<span style="color:#666">.</span>create(x<span style="color:#666">.</span>data, sizes))
</code></pre></div><ul>
<li>
<h2 id="realize">realize</h2>
</li>
</ul>
<p>Inductor IR的设计中，<code>TensorBox</code>都是逻辑节点（简单的理解：是存在于寄存器中的变量，不会真的为其分配RAM用于实例化，想要拿到这个节点的result必须要按照inner_fn描述的计算逻辑算一遍），只有在realize后这个节点才会被分配内存（后续节点要访问这个节点的result，可以直接从内存中获取结果，而非recompute）。Inductor对于节点的realize从代码上看不是结构化的，而是散落在很多地方。</p>
<p>在lowering的实现中，有些ops会显示的要求realize，如：</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f">@register_lowering</span>(aten<span style="color:#666">.</span>bernoulli_, type_promotion_kind<span style="color:#666">=</span>None)
<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">bernoulli_</span>(x, <span style="color:#666">*</span>args):
    <span style="color:#a2f;font-weight:bold">assert</span> config<span style="color:#666">.</span>fallback_random <span style="color:#a2f;font-weight:bold">or</span> x<span style="color:#666">.</span>get_device() <span style="color:#666">==</span> torch<span style="color:#666">.</span>device(
        <span style="color:#b44">&#34;cpu&#34;</span>
    ), <span style="color:#b44">&#34;this should be handled in decomps unless config.fallback_random or the device is CPU&#34;</span>
    x<span style="color:#666">.</span>realize()
    ir<span style="color:#666">.</span>InplaceBernoulliFallback(x, <span style="color:#666">*</span>args)
    <span style="color:#a2f;font-weight:bold">return</span> x
</code></pre></div><h1 id="ir">ir</h1>
<p><code>GraphLowering.run</code>通过调用注册在<code>lowerings</code>中的实现，将aten ops转换为 Inductor IR.</p>
<ul>
<li>
<h2 id="inductor-ir的设计">Inductor IR的设计</h2>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#b44">&#34;&#34;&#34; [Note: Inductor IR]
</span><span style="color:#b44">
</span><span style="color:#b44">Inductor&#39;s IR is produced by executing &#39;lowering&#39; code (see lowering.py).  Each
</span><span style="color:#b44">lowering is registered to a particular aten operator, and expects inputs that
</span><span style="color:#b44">correspond to the aten schema.  However, in place of torch Tensor inputs, lowerings
</span><span style="color:#b44">expect Inductor TensorBox inputs.
</span><span style="color:#b44">
</span><span style="color:#b44">TensorBox IR represents torch tensors.  Tensors are sometimes single objects owning
</span><span style="color:#b44">storage, and sometimes views of another Tensor&#39;s storage.  Mutating tensor operations
</span><span style="color:#b44">(such as add_()) affect the underlying storage and any associated views.  Other operations
</span><span style="color:#b44">(such as .t_()) update metadata about the current view but don&#39;t modify the underlying storage.
</span><span style="color:#b44">
</span><span style="color:#b44">To model this in Inductor, the IR distinguishes between TensorBox, View, StorageBox and Buffer.
</span><span style="color:#b44">
</span><span style="color:#b44">TensorBox is the top level IR construct that any lowering should produce and maps to a torch.Tensor
</span><span style="color:#b44">output from an operation.  But just as torch.Tensors take different forms, TensorBox IR can
</span><span style="color:#b44">reference View IR or directly reference StorageBox IRs.
</span><span style="color:#b44">
</span><span style="color:#b44">Some Inductor lowerings produce new sets of &#39;Box&#39;es, while others (such as .t() or other view ops)
</span><span style="color:#b44">may take an existing TensorBox and point it to a new underlying View IR.
</span><span style="color:#b44">
</span><span style="color:#b44">Tensors that directly own storage are represented as a chain of:
</span><span style="color:#b44">TensorBox -&gt; StorageBox -&gt; Buffer
</span><span style="color:#b44">where Buffer is a simple (1D) allocation, and StorageBox introduces the concept of a Layout.
</span><span style="color:#b44">
</span><span style="color:#b44">If you mutate the data of such a tensor, we swing the StorageBox pointer to point to a new buffer
</span><span style="color:#b44">(leaving the old buffer unmodified and functionalizing the operation).
</span><span style="color:#b44">
</span><span style="color:#b44">Tensors backed by views add one more indirection to the IR.
</span><span style="color:#b44">TensorBox -&gt; View -&gt; StorageBox -&gt; Buffer
</span><span style="color:#b44">In these cases, the underlying StorageBox/Buffer will be shared with the pre-view TensorBox.
</span><span style="color:#b44">&#34;&#34;&#34;</span>
</code></pre></div><p>Inductor IR主要包含<code>Buffer</code>、<code>Loops</code>、<code>TensorBox</code> 对计算图进行建模。其中，
（1）<code>TensorBox</code>对位的是torch的tensor，是tensor的抽象表达，表示每个node产生的结果，<code>StorageBox</code>对位的是torch tensor的storage，表示真实的tensor对象，<code>StorageBox</code>对象会对应一个<code>Buffer</code>，所以 TensorBox -&gt; StorageBox -&gt; Buffer。torch中的viewlike-ops不会产生新的storage，而是一个view，Inductor IR 用 <code>View</code>去表达这种tensor，即与已有的tensor共享storage的类型，所以也有TensorBox -&gt; View -&gt; StorageBox -&gt; Buffer
（2）<code>Buffer</code>对应的是实际物理内存，可以派生出<code>InputBuffer</code>，<code>ComputedBuffer</code>，<code>InputsKernekl</code>，<code>TemplateBuffer</code>，<code>ExternKernel</code>等。codegen的输入是<code>Buffer</code>s，每个buffer节点对应一个kernel或外部函数调用。特别的，<code>ComputedBuffer</code>中的data成员对象是<code>Loops</code>类型，表示当前<code>ComputedBuffer</code>做了哪些计算逻辑。
（3）<code>Loops</code>是计算逻辑的抽象，其派生出<code>Pointwise</code>，<code>Reduction</code>等，对不同类型ops进行建模。<code>Loops</code>对象存在于<code>ComputedBuffer</code>中，表示这个buffer会进行的计算逻辑。</p>
<p>TODO 补一张基于IRNode派生的类的关系图。</p>
<ul>
<li>
<h2 id="realize-1">realize</h2>
</li>
</ul>
<p>IRNode在realize之前是可以fuse的，realize之后结果会写到真实的内存，即当前节点的result可以直接从内存中获取，而不需要重计算出这个结果。所以可以推测IRNode的设计，以realize的IRNode作为codegen对象，这个IRNode的计算逻辑记录在<code>Loops</code>的<code>inner_fn</code>对象中。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">realize</span>(self):
        <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">        If the IRNode refers to data which has not been materialized (e.g.,
</span><span style="color:#b44">        it is a Pointwise/Reduction that could potentially have more
</span><span style="color:#b44">        compute fused into it), realize the IRNode into physical memory,
</span><span style="color:#b44">        ending the possibility of fusing into it, but allowing, e.g., multiple
</span><span style="color:#b44">        users to access the data without having to recompute.
</span><span style="color:#b44">
</span><span style="color:#b44">        Check StorageBox.realize for a particularly notable implementation.
</span><span style="color:#b44">
</span><span style="color:#b44">        TODO(ezyang): I think, in principle, every IRNode should have an
</span><span style="color:#b44">        implementation of this, and most of the time no-op is OK, but you
</span><span style="color:#b44">        really do have to audit each IRNode for this, so for now, raise
</span><span style="color:#b44">        an error if it&#39;s not implemented.  Note that some code in graph.py
</span><span style="color:#b44">        will catch this thrown error and suppress it with a warning.
</span><span style="color:#b44">        &#34;&#34;&#34;</span>
</code></pre></div><p>那么在合适的位置（tensor）进行realize则是inductor在lowering过程中的核心。整体上看，应该在什么地方realize并没有一个很统一的处理，而是分散在很多地方，但其原则是尽量用重计算减少访存开销。</p>
<ul>
<li>
<h3 id="reduce-op-会进行realize">reduce op 会进行realize</h3>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">make_reduction</span>(reduction_type: <span style="color:#a2f">str</span>, override_return_dtype<span style="color:#666">=</span>None):
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">inner</span>(x, axis<span style="color:#666">=</span>None, keepdims<span style="color:#666">=</span>False, <span style="color:#666">*</span>, dtype<span style="color:#666">=</span>None):
        kwargs <span style="color:#666">=</span> _make_reduction_inner(
            x,
            axis<span style="color:#666">=</span>axis,
            keepdims<span style="color:#666">=</span>keepdims,
            dtype<span style="color:#666">=</span>dtype,
            override_return_dtype<span style="color:#666">=</span>override_return_dtype,
        )
        result <span style="color:#666">=</span> Reduction<span style="color:#666">.</span>create(reduction_type<span style="color:#666">=</span>reduction_type, <span style="color:#666">**</span>kwargs)
        <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(
            result<span style="color:#666">.</span>data<span style="color:#666">.</span>data, Reduction
        ):  <span style="color:#080;font-style:italic"># Only realize if reduction isn&#39;t unrolled</span>
            result<span style="color:#666">.</span>realize()
        <span style="color:#a2f;font-weight:bold">return</span> result

    <span style="color:#a2f;font-weight:bold">return</span> inner
</code></pre></div><ul>
<li>
<h3 id="graphlowering-2">GraphLowering</h3>
</li>
</ul>
<p>在<code>run_nodes</code>中，如果（1）users要求当前节点需要realize（2）当前节点的被读的次数超出阈值，recompute代价较大。当满足其中一个条件则会realize。</p>
<ul>
<li>
<h3 id="op-lowering">op lowering</h3>
</li>
</ul>
<p>在lowering aten ops到Inductor IR时，一些实现会调用<code>realize</code>或<code>make_reuse</code>进行realize</p>
<ul>
<li>
<h3 id="storageboxrealize">StorageBox.realize</h3>
</li>
</ul>
<p>realize实则是实例化了self.data对象（ComputedBuffer），注意到实例化<code>ComputedBuffer</code>时传入的self.data是<code>StorageBox</code>自身的data成员对象，也就说<code>StorageBox.data</code>在realize之前是<code>Loops</code>对象，之后是<code>ComputedBuffer</code></p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">realize</span>(self):
        <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(
            self<span style="color:#666">.</span>data,
            (
                ComputedBuffer,
                InputsKernel,
                InputBuffer,
                ReinterpretView,
                TemplateBuffer,
            ),
        ):
            <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>data<span style="color:#666">.</span>get_name()
        <span style="color:#a2f;font-weight:bold">assert</span> <span style="color:#a2f">isinstance</span>(self<span style="color:#666">.</span>data, (Pointwise, Reduction)), <span style="color:#a2f">type</span>(self<span style="color:#666">.</span>data)
        origin_node <span style="color:#666">=</span> self<span style="color:#666">.</span>data<span style="color:#666">.</span>get_origin_node()
        traceback <span style="color:#666">=</span> self<span style="color:#666">.</span>data<span style="color:#666">.</span>get_traceback()
        self<span style="color:#666">.</span>data <span style="color:#666">=</span> ComputedBuffer(
            name<span style="color:#666">=</span>None,
            layout<span style="color:#666">=</span>FlexibleLayout(
                device<span style="color:#666">=</span>self<span style="color:#666">.</span>data<span style="color:#666">.</span>get_device(),
                dtype<span style="color:#666">=</span>self<span style="color:#666">.</span>data<span style="color:#666">.</span>get_dtype(),
                size<span style="color:#666">=</span>self<span style="color:#666">.</span>data<span style="color:#666">.</span>get_size(),
            ),
            data<span style="color:#666">=</span>self<span style="color:#666">.</span>data,
        )
        self<span style="color:#666">.</span>data<span style="color:#666">.</span>name <span style="color:#666">=</span> V<span style="color:#666">.</span>graph<span style="color:#666">.</span>register_buffer(self<span style="color:#666">.</span>data)
        self<span style="color:#666">.</span>data<span style="color:#666">.</span>origins <span style="color:#666">=</span> self<span style="color:#666">.</span>origins
        self<span style="color:#666">.</span>data<span style="color:#666">.</span>origin_node <span style="color:#666">=</span> origin_node
        self<span style="color:#666">.</span>data<span style="color:#666">.</span>traceback <span style="color:#666">=</span> traceback
        <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>data<span style="color:#666">.</span>name
</code></pre></div><ul>
<li>
<h2 id="loopbody">LoopBody</h2>
</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">LoopBody</span>:
    <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">    Captures the body of a Loops subclass into an FX graph.  Persists any
</span><span style="color:#b44">    indexing simplifications and makes it easier to analyze loop bodies.
</span><span style="color:#b44">    &#34;&#34;&#34;</span>
</code></pre></div><p>是用于拼接Loops的辅助类，如在<code>simplify_and_reorder</code>函数中将当前节点（loop body）拼接进ComputedBuffer的loop body中。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">simplify_and_reorder</span>(self):
        <span style="color:#b44">&#34;&#34;&#34;
</span><span style="color:#b44">        This is a main place where we do loop transformations in a
</span><span style="color:#b44">        backend-agnostic way.
</span><span style="color:#b44">
</span><span style="color:#b44">        Here we:
</span><span style="color:#b44">            1) Remove any 1 dimensions
</span><span style="color:#b44">            2) Fuse contiguous dimensions together
</span><span style="color:#b44">            3) Reorder dimensions based on stride orders
</span><span style="color:#b44">        &#34;&#34;&#34;</span>
</code></pre></div><h1 id="decompositions">Decompositions</h1>
<ul>
<li>
<h2 id="core_aten_decompositions">core_aten_decompositions</h2>
</li>
</ul>
<p>定义<code>pytorch/torch/_decomp/init.py</code>中，是从aten op分解的实现中筛选出的由CoreAten Ops实现的集合。</p>
<ul>
<li>
<h2 id="get_decompositions">get_decompositions</h2>
</li>
</ul>
<p><code>register_lowering</code>会将aten ops的分解的实现注册到<code>global_decomposition_table</code>，get_decompositions则是根据输入的aten ops，返回对应的注册的实现。</p>

    </div>

    
        <div class="tags">
            
                <a href="https://yellowhch.github.io/tags/torch">Torch</a>
            
                <a href="https://yellowhch.github.io/tags/inductor">Inductor</a>
            
                <a href="https://yellowhch.github.io/tags/graph-compiler">Graph compiler</a>
            
        </div>
    
    
    

</section>


    </main>
    
    <footer id="footer">
    
        <div id="social">


    <a class="symbol" href="https://github.com/YellowHCH/" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28"  viewBox="0 0 72 72" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    
    <title>Github</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)">
            <g id="Github" transform="translate(264.000000, 939.000000)">
                <path d="M8,72 L64,72 C68.418278,72 72,68.418278 72,64 L72,8 C72,3.581722 68.418278,-8.11624501e-16 64,0 L8,0 C3.581722,8.11624501e-16 -5.41083001e-16,3.581722 0,8 L0,64 C5.41083001e-16,68.418278 3.581722,72 8,72 Z" id="Rounded" fill="#bbbbbb"></path>
                <path d="M35.9985,13 C22.746,13 12,23.7870921 12,37.096644 C12,47.7406712 18.876,56.7718301 28.4145,59.9584121 C29.6145,60.1797862 30.0525,59.4358488 30.0525,58.7973276 C30.0525,58.2250681 30.0315,56.7100863 30.0195,54.6996482 C23.343,56.1558981 21.9345,51.4693938 21.9345,51.4693938 C20.844,48.6864054 19.2705,47.9454799 19.2705,47.9454799 C17.091,46.4500754 19.4355,46.4801943 19.4355,46.4801943 C21.843,46.6503662 23.1105,48.9634994 23.1105,48.9634994 C25.2525,52.6455377 28.728,51.5823398 30.096,50.9649018 C30.3135,49.4077535 30.9345,48.3460615 31.62,47.7436831 C26.2905,47.1352808 20.688,45.0691228 20.688,35.8361671 C20.688,33.2052792 21.6225,31.0547881 23.1585,29.3696344 C22.911,28.7597262 22.0875,26.3110578 23.3925,22.9934585 C23.3925,22.9934585 25.4085,22.3459017 29.9925,25.4632101 C31.908,24.9285993 33.96,24.6620468 36.0015,24.6515052 C38.04,24.6620468 40.0935,24.9285993 42.0105,25.4632101 C46.5915,22.3459017 48.603,22.9934585 48.603,22.9934585 C49.9125,26.3110578 49.089,28.7597262 48.8415,29.3696344 C50.3805,31.0547881 51.309,33.2052792 51.309,35.8361671 C51.309,45.0917119 45.6975,47.1292571 40.3515,47.7256117 C41.2125,48.4695491 41.9805,49.9393525 41.9805,52.1877301 C41.9805,55.4089489 41.9505,58.0067059 41.9505,58.7973276 C41.9505,59.4418726 42.3825,60.1918338 43.6005,59.9554002 C53.13,56.7627944 60,47.7376593 60,37.096644 C60,23.7870921 49.254,13 35.9985,13" fill="#FFFFFF"></path>
            </g>
        </g>
    </g>
</svg>
    </a>


</div>

    

    <div class="copyright">
    
       © Copyright 
       2024 
       <span class="split">
        <svg fill="#bbbbbb" width="15" height="15" version="1.1" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15px" height="15px" viewBox="0 0 15 15">
  <path d="M13.91,6.75c-1.17,2.25-4.3,5.31-6.07,6.94c-0.1903,0.1718-0.4797,0.1718-0.67,0C5.39,12.06,2.26,9,1.09,6.75&#xA;&#x9;C-1.48,1.8,5-1.5,7.5,3.45C10-1.5,16.48,1.8,13.91,6.75z"/>
</svg>
       </span>
       ChenhuiHuang
    
    </div>

    
</footer>



  </body>
</html>
