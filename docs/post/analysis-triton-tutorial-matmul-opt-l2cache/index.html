<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>Analysis triton tutorial matmul L2 cache optimization - </title>

  <meta name="description" content="Ref to triton tutorial
 triton compiler 负责CTA内部的线程排布以及内存排布，CTA外部（即如何排布CTA）是由使用者去tune的。这篇triton的教程介绍了如何提高基于GPU 缓存的data reuse。 在GPU架构中，L1 cache是SM内的，L2 cache是全局的，所以基于L1的优化是triton compiler的事情，L2是用户去考虑的。 基于triton的mm的伪代码 实现如下：
# Do in parallel for m in range(0, M, BLOCK_SIZE_M): # Do in parallel for n in range(0, N, BLOCK_SIZE_N): acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32) for k in range(0, K, BLOCK_SIZE_K): a = A[m : m&#43;BLOCK_SIZE_M, k : k&#43;BLOCK_SIZE_K] b = B[k : k&#43;BLOCK_SIZE_K, n : n&#43;BLOCK_SIZE_N] acc &#43;= dot(a, b) C[m : m&#43;BLOCK_SIZE_M, n : n&#43;BLOCK_SIZE_N] = acc 逻辑上每个CTA的执行是并行的，但实际上，每个CTA都会放在SM上执行，物理上不一定是完全并行的，因此CTA的排布（执行编号）可能会影响cache hit（试想，如果某时刻SMs上的CTA从完全不同的RAM中读数据，那么cache miss是会很严重，因此起不到data reuse的效果；反之，如果SMs之间会读取相同的内存块，那么cache hit就会提高，不用频繁从RAM中读数据）。 为方便分析，不妨假设SM数量为9，对于一个分块后tile_m=tile_n=9的mm，同时会有9个CTA在SM上执行，即同时计算C的9个blocks，最简单的CTA layout是根据raw-major去排布，如下代码所示：">
  <meta name="author" content="Chenhui Huang"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "ChenhuiHuang\x27s Blog",
    
    "url": "https:\/\/yellowhch.github.io"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/yellowhch.github.io"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/yellowhch.github.io",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/yellowhch.github.io\/post\/analysis-triton-tutorial-matmul-opt-l2cache\/",
          "name": "Analysis triton tutorial matmul l2 cache optimization"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "Chenhui Huang"
  },
  "headline": "Analysis triton tutorial matmul L2 cache optimization",
  "description" : "Ref to triton tutorial\n triton compiler 负责CTA内部的线程排布以及内存排布，CTA外部（即如何排布CTA）是由使用者去tune的。这篇triton的教程介绍了如何提高基于GPU 缓存的data reuse。 在GPU架构中，L1 cache是SM内的，L2 cache是全局的，所以基于L1的优化是triton compiler的事情，L2是用户去考虑的。 基于triton的mm的伪代码 实现如下：\n# Do in parallel for m in range(0, M, BLOCK_SIZE_M): # Do in parallel for n in range(0, N, BLOCK_SIZE_N): acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32) for k in range(0, K, BLOCK_SIZE_K): a = A[m : m\x2bBLOCK_SIZE_M, k : k\x2bBLOCK_SIZE_K] b = B[k : k\x2bBLOCK_SIZE_K, n : n\x2bBLOCK_SIZE_N] acc \x2b= dot(a, b) C[m : m\x2bBLOCK_SIZE_M, n : n\x2bBLOCK_SIZE_N] = acc 逻辑上每个CTA的执行是并行的，但实际上，每个CTA都会放在SM上执行，物理上不一定是完全并行的，因此CTA的排布（执行编号）可能会影响cache hit（试想，如果某时刻SMs上的CTA从完全不同的RAM中读数据，那么cache miss是会很严重，因此起不到data reuse的效果；反之，如果SMs之间会读取相同的内存块，那么cache hit就会提高，不用频繁从RAM中读数据）。 为方便分析，不妨假设SM数量为9，对于一个分块后tile_m=tile_n=9的mm，同时会有9个CTA在SM上执行，即同时计算C的9个blocks，最简单的CTA layout是根据raw-major去排布，如下代码所示：",
  "inLanguage" : "en",
  "wordCount":  270 ,
  "datePublished" : "2023-12-15T00:00:00",
  "dateModified" : "2023-12-15T00:00:00",
  "image" : "https:\/\/yellowhch.github.io\/img\/avatar-icon.png",
  "keywords" : [ "triton, matmul" ],
  "mainEntityOfPage" : "https:\/\/yellowhch.github.io\/post\/analysis-triton-tutorial-matmul-opt-l2cache\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/yellowhch.github.io",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/yellowhch.github.io\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Analysis triton tutorial matmul L2 cache optimization" />
<meta property="og:description" content="Ref to triton tutorial
 triton compiler 负责CTA内部的线程排布以及内存排布，CTA外部（即如何排布CTA）是由使用者去tune的。这篇triton的教程介绍了如何提高基于GPU 缓存的data reuse。 在GPU架构中，L1 cache是SM内的，L2 cache是全局的，所以基于L1的优化是triton compiler的事情，L2是用户去考虑的。 基于triton的mm的伪代码 实现如下：
# Do in parallel for m in range(0, M, BLOCK_SIZE_M): # Do in parallel for n in range(0, N, BLOCK_SIZE_N): acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32) for k in range(0, K, BLOCK_SIZE_K): a = A[m : m&#43;BLOCK_SIZE_M, k : k&#43;BLOCK_SIZE_K] b = B[k : k&#43;BLOCK_SIZE_K, n : n&#43;BLOCK_SIZE_N] acc &#43;= dot(a, b) C[m : m&#43;BLOCK_SIZE_M, n : n&#43;BLOCK_SIZE_N] = acc 逻辑上每个CTA的执行是并行的，但实际上，每个CTA都会放在SM上执行，物理上不一定是完全并行的，因此CTA的排布（执行编号）可能会影响cache hit（试想，如果某时刻SMs上的CTA从完全不同的RAM中读数据，那么cache miss是会很严重，因此起不到data reuse的效果；反之，如果SMs之间会读取相同的内存块，那么cache hit就会提高，不用频繁从RAM中读数据）。 为方便分析，不妨假设SM数量为9，对于一个分块后tile_m=tile_n=9的mm，同时会有9个CTA在SM上执行，即同时计算C的9个blocks，最简单的CTA layout是根据raw-major去排布，如下代码所示：">
<meta property="og:image" content="https://yellowhch.github.io/img/avatar-icon.png" />
<meta property="og:url" content="https://yellowhch.github.io/post/analysis-triton-tutorial-matmul-opt-l2cache/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="ChenhuiHuang&#39;s Blog" />

  <meta name="twitter:title" content="Analysis triton tutorial matmul L2 cache optimization" />
  <meta name="twitter:description" content="Ref to triton tutorial
 triton compiler 负责CTA内部的线程排布以及内存排布，CTA外部（即如何排布CTA）是由使用者去tune的。这篇triton的教程介绍了如何提高基于GPU 缓存的data reuse。 在GPU架构中，L1 cache是SM内的，L2 cache是全局的，所以基于L1的优化是triton compiler的事情，L2是用户去考虑的。  …">
  <meta name="twitter:image" content="https://yellowhch.github.io/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='https://yellowhch.github.io/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.68.3" />
  <link rel="alternate" href="https://yellowhch.github.io/index.xml" type="application/rss+xml" title="ChenhuiHuang&#39;s Blog"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://yellowhch.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://yellowhch.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://yellowhch.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">


  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://yellowhch.github.io">ChenhuiHuang&#39;s Blog</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/page/about/">About</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="ChenhuiHuang&#39;s Blog" href="https://yellowhch.github.io">
            <img class="avatar-img" src="https://yellowhch.github.io/img/avatar-icon.png" alt="ChenhuiHuang&#39;s Blog" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>Analysis triton tutorial matmul L2 cache optimization</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on map[Count:December 15, 2023]
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;270&nbsp;words
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;Chenhui Huang
    
  
  
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>Ref to <a href="https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html">triton tutorial</a></p>
<hr>
<p>triton compiler 负责CTA内部的线程排布以及内存排布，CTA外部（即如何排布CTA）是由使用者去tune的。这篇triton的教程介绍了如何提高基于GPU 缓存的data reuse。
在GPU架构中，L1 cache是SM内的，L2 cache是全局的，所以基于L1的优化是triton compiler的事情，L2是用户去考虑的。
基于triton的mm的伪代码 实现如下：</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Do in parallel</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_M</span><span class="p">):</span>
  <span class="c1"># Do in parallel</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">):</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">):</span>
      <span class="n">a</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span> <span class="p">:</span> <span class="n">m</span><span class="o">+</span><span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">k</span> <span class="p">:</span> <span class="n">k</span><span class="o">+</span><span class="n">BLOCK_SIZE_K</span><span class="p">]</span>
      <span class="n">b</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="p">:</span> <span class="n">k</span><span class="o">+</span><span class="n">BLOCK_SIZE_K</span><span class="p">,</span> <span class="n">n</span> <span class="p">:</span> <span class="n">n</span><span class="o">+</span><span class="n">BLOCK_SIZE_N</span><span class="p">]</span>
      <span class="n">acc</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">C</span><span class="p">[</span><span class="n">m</span> <span class="p">:</span> <span class="n">m</span><span class="o">+</span><span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">n</span> <span class="p">:</span> <span class="n">n</span><span class="o">+</span><span class="n">BLOCK_SIZE_N</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc</span>
</code></pre></div><p>逻辑上每个CTA的执行是并行的，但实际上，每个CTA都会放在SM上执行，物理上不一定是完全并行的，因此CTA的排布（执行编号）可能会影响cache hit（试想，如果某时刻SMs上的CTA从完全不同的RAM中读数据，那么cache miss是会很严重，因此起不到data reuse的效果；反之，如果SMs之间会读取相同的内存块，那么cache hit就会提高，不用频繁从RAM中读数据）。
为方便分析，不妨假设SM数量为9，对于一个分块后tile_m=tile_n=9的mm，同时会有9个CTA在SM上执行，即同时计算C的9个blocks，最简单的CTA layout是根据raw-major去排布，如下代码所示：</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">pid</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">grid_m</span> <span class="o">=</span> <span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">BLOCK_SIZE_M</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">BLOCK_SIZE_M</span><span class="p">;</span>
<span class="n">grid_n</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">BLOCK_SIZE_N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">BLOCK_SIZE_N</span><span class="p">;</span>
<span class="n">pid_m</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">/</span> <span class="n">grid_n</span><span class="p">;</span>
<span class="n">pid_n</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">%</span> <span class="n">grid_n</span><span class="p">;</span>
</code></pre></div><p>pid 是CTA的编号，往往也是SM调度的顺序。pid_m和pid_n是当前编号pid的CTA对应的需要计算的block，其效果如下图：</p>
<p><img src="/images/mm-l2cache-00.png" alt="image"></p>
<p>即CTA的layout（CTA与block的映射关系）是行主序的。为了计算C的9个block，如果没有cache，那么需要load A的9个block 9次，B的9个block 9次，即一共load 162次 block。但因为有 L2 cache，SM从A中访问的是相同内存，所以会复用cache的数据，所以实际对A的9个block只从global memory 读了一次，但不同SM从B中读的blocks都是不同的，无法利用L2 cache，所以需要从B中读81个block，AB合计读90个block。
为了提高L2 cache hit rate，容易想到调整当前9个CTA对应的layout，尽量提高cache hit。
triton教程介绍的是名为 <strong>Grouped ordering</strong> 的排布方式，如下图所示:</p>
<p><img src="/images/mm-l2cache-01.png" alt="image"></p>
<p>如果物理上每个时刻有9个SM并发，那么此时从A和B中读的数据都会被reuse 2次，即读block的数量减少了2/3，优化成了162/3=54次。
计算CTA的layout的伪代码如下：</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Program ID</span>
<span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Number of program ids along the M axis</span>
<span class="n">num_pid_m</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_M</span><span class="p">)</span>
<span class="c1"># Number of programs ids along the N axis</span>
<span class="n">num_pid_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">)</span>
<span class="c1"># Number of programs in group</span>
<span class="n">num_pid_in_group</span> <span class="o">=</span> <span class="n">GROUP_SIZE_M</span> <span class="o">*</span> <span class="n">num_pid_n</span>
<span class="c1"># Id of the group this program is in</span>
<span class="n">group_id</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">//</span> <span class="n">num_pid_in_group</span>
<span class="c1"># Row-id of the first program in the group</span>
<span class="n">first_pid_m</span> <span class="o">=</span> <span class="n">group_id</span> <span class="o">*</span> <span class="n">GROUP_SIZE_M</span>
<span class="c1"># If `num_pid_m` isn&#39;t divisible by `GROUP_SIZE_M`, the last group is smaller</span>
<span class="n">group_size_m</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_pid_m</span> <span class="o">-</span> <span class="n">first_pid_m</span><span class="p">,</span> <span class="n">GROUP_SIZE_M</span><span class="p">)</span>
<span class="c1"># *Within groups*, programs are ordered in a column-major order</span>
<span class="c1"># Row-id of the program in the *launch grid*</span>
<span class="n">pid_m</span> <span class="o">=</span> <span class="n">first_pid_m</span> <span class="o">+</span> <span class="p">(</span><span class="n">pid</span> <span class="o">%</span> <span class="n">group_size_m</span><span class="p">)</span>
<span class="c1"># Col-id of the program in the *launch grid*</span>
<span class="n">pid_n</span> <span class="o">=</span> <span class="p">(</span><span class="n">pid</span> <span class="o">%</span> <span class="n">num_pid_in_group</span><span class="p">)</span> <span class="o">//</span> <span class="n">group_size_m</span>
</code></pre></div><p>每GROUP_SIZE_M行为一个Group，每个group内是列主序的。因此layout要先计算当前pid映射到哪个group，然后计算在group中的位置。其中，超参GROUP_SIZE_M需要tune，显然这个参数与L2 cache的size以及SM数量有关。triton教程中将GROUP_SIZE_M设置成8，A100有108个SM，因此预设的是每次SMs计算一个8x?的blocks。按理说应该计算sqrt(num_sm)xsqrt(num_sm)比较合适，约等于10x10，所以将GROUP_SIZE_M设置成8也算合理。</p>


        
          <div class="blog-tags">
            
              <a href="https://yellowhch.github.io/tags/triton/">triton</a>&nbsp;
            
              <a href="https://yellowhch.github.io/tags/matmul/">matmul</a>&nbsp;
            
          </div>
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fyellowhch.github.io%2fpost%2fanalysis-triton-tutorial-matmul-opt-l2cache%2f&amp;text=Analysis%20triton%20tutorial%20matmul%20L2%20cache%20optimization&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fyellowhch.github.io%2fpost%2fanalysis-triton-tutorial-matmul-opt-l2cache%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fyellowhch.github.io%2fpost%2fanalysis-triton-tutorial-matmul-opt-l2cache%2f&amp;title=Analysis%20triton%20tutorial%20matmul%20L2%20cache%20optimization" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fyellowhch.github.io%2fpost%2fanalysis-triton-tutorial-matmul-opt-l2cache%2f&amp;title=Analysis%20triton%20tutorial%20matmul%20L2%20cache%20optimization" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fyellowhch.github.io%2fpost%2fanalysis-triton-tutorial-matmul-opt-l2cache%2f&amp;title=Analysis%20triton%20tutorial%20matmul%20L2%20cache%20optimization" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fyellowhch.github.io%2fpost%2fanalysis-triton-tutorial-matmul-opt-l2cache%2f&amp;description=Analysis%20triton%20tutorial%20matmul%20L2%20cache%20optimization" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
          
        </ul>
      


      
        
        
      

    </div>
  </div>
</div>

      
<footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
		
		  <a href="mailto:ch_huang@zju.edu.cn" title="Email me">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://github.com/YellowHCH" title="GitHub">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          <li>
            <a href="" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="https://yellowhch.github.io">Chenhui Huang</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2023
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://yellowhch.github.io">ChenhuiHuang&#39;s Blog</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.68.3</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://yellowhch.github.io/js/main.js"></script>
<script src="https://yellowhch.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://yellowhch.github.io/js/load-photoswipe.js"></script>









    
  </body>
</html>

