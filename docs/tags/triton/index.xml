<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>triton on ChenhuiHuang&#39;s Blog</title>
    <link>https://yellowhch.github.io/tags/triton/</link>
    <description>Recent content in triton on ChenhuiHuang&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>ch_huang@zju.edu.cn (Chenhui Huang)</managingEditor>
    <webMaster>ch_huang@zju.edu.cn (Chenhui Huang)</webMaster>
    <lastBuildDate>Fri, 15 Dec 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://yellowhch.github.io/tags/triton/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Analysis triton tutorial matmul L2 cache optimization</title>
      <link>https://yellowhch.github.io/post/analysis-triton-tutorial-matmul-opt-l2cache/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <author>ch_huang@zju.edu.cn (Chenhui Huang)</author>
      <guid>https://yellowhch.github.io/post/analysis-triton-tutorial-matmul-opt-l2cache/</guid>
      <description>Ref to triton tutorial
 triton compiler 负责CTA内部的线程排布以及内存排布，CTA外部（即如何排布CTA）是由使用者去tune的。这篇triton的教程介绍了如何提高基于GPU 缓存的data reuse。 在GPU架构中，L1 cache是SM内的，L2 cache是全局的，所以基于L1的优化是triton compiler的事情，L2是用户去考虑的。 基于triton的mm的伪代码 实现如下：
# Do in parallel for m in range(0, M, BLOCK_SIZE_M): # Do in parallel for n in range(0, N, BLOCK_SIZE_N): acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32) for k in range(0, K, BLOCK_SIZE_K): a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K] b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N] acc += dot(a, b) C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc 逻辑上每个CTA的执行是并行的，但实际上，每个CTA都会放在SM上执行，物理上不一定是完全并行的，因此CTA的排布（执行编号）可能会影响cache hit（试想，如果某时刻SMs上的CTA从完全不同的RAM中读数据，那么cache miss是会很严重，因此起不到data reuse的效果；反之，如果SMs之间会读取相同的内存块，那么cache hit就会提高，不用频繁从RAM中读数据）。 为方便分析，不妨假设SM数量为9，对于一个分块后tile_m=tile_n=9的mm，同时会有9个CTA在SM上执行，即同时计算C的9个blocks，最简单的CTA layout是根据raw-major去排布，如下代码所示：</description>
    </item>
    
  </channel>
</rss>